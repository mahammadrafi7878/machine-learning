{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c01ffc95",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "raw",
   "id": "29965053",
   "metadata": {},
   "source": [
    "Feature engineering is the proces of selecting,manipulating and transforming raw data into features that can be used in supervised machine lerning , In order to make machine learning work well on new tasks.\n",
    "Feature engineering in simple terms , Is the act of converting raw observations into desired features by using statistical or ML process.\n",
    "Feature learning is a machine learning technique that leverages data to create new variables that are not in the training set . It can produce new features for both supervised and unsupeervised learning with the goal of simplifying and speeding up data transformations will aals enhancing model accuracy.\n",
    "Feature engineering consists of various process \n",
    "\n",
    "\n",
    "FEATURE CREATION: \n",
    "     Creating features involves creating new variables which will be most helpful for model.This can be adding or removing some features.\n",
    "\n",
    "FEATURE TRANSFORMATION: \n",
    "       Feature Transformation is simply a function that transforms features from one representationto another representation.\n",
    "\n",
    "FEATURE EXTRACTION: \n",
    "          Feature extraction is the process of extracting features from a dataset to identify useful information with out disturbing the original relation ships.\n",
    "          \n",
    "EDA(Exploratory Data Analysis):\n",
    "    EDA is a power ful and simplke tool that can be used to improve you are understanding of the data , by exploring it's properties.\n",
    "    \n",
    "BENCH MARK:\n",
    "     A bench mark model is the most user friendly dependable ,transperant and interpretable model.\n",
    "     It's a good idea to run test data sets to see if your new ML model out performs as recognised bench mark.\n",
    "     \n",
    " Some feature engineering techniques Imputation, Handling outliers, Log transformation ,One hot Encoding , Scaling and standardization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "003a7c69",
   "metadata": {},
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28b5d0c2",
   "metadata": {},
   "source": [
    "Feature selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data.\n",
    "  \n",
    "  When we have a 100+ features in a dataset that is ridiculous cannot to process normally which where feature selection methods come in handy,\n",
    "They allow you to reduce the number of features included in a model without sacrifising the predictive power.\n",
    "Features that are redundant or irrelevent can actually negatively impact your model performance , So it is necessary to remove them.\n",
    "There are three types of feature selection .\n",
    "\n",
    "1.WRAPPER METHODS(Forword, back word and stepwise).\n",
    "2.FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold)\n",
    "3.ENSEMBLE METHODS(Lasso,Ridge and Decision tree)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfb0b19c",
   "metadata": {},
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "499c4b77",
   "metadata": {},
   "source": [
    "Feature selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. \n",
    "\n",
    "\n",
    "FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold):\n",
    "    filter methods use a measure rather than error rate to determine whether the feature is useful rather than knowing a model , a subset of the features is selected through ranking them by a useful descriptive mesure.\n",
    "    ADVANTAGE: Benfits of filter methods That they have a very low computation time and will not overfit the data.\n",
    "    DRAW BACK: However One draw back is that they are blind to any interactions or correlation between features.This will need to taken into account seperately, some filter method are ANOVA,Pearson's correlation co-efficient and Variance Threshold.\n",
    "    \n",
    " 1. ANOVA(Analysis of variance):\n",
    "     The ANOVA test works with the variation within the treatments of a feature and also between the treatments.\n",
    "     These variance are important metrices for specified Filter method, Because we can determine whether a feature does a good job accounting for variation in the dependent variable.\n",
    "     An ANNOVA test has F-statistics is computed for each individual feature with the variation between  traetments in the numeric and the variation with in treatments in denominator.\n",
    "     \n",
    "  2.Pearson's correlation Coefficient:\n",
    "         Pearson correlation coefficient is a measure of the similarity of two features that ranges between -1 to 1.\n",
    "         A value close to 1 or -1 indicates that the features have a high correlation and may be relatedthe coefficients of high correlation vs low correlation depends on the range of correlation coefficients with in each dataset.\n",
    "         \n",
    "  3.VARIANCE THRESHOLDING: \n",
    "        The variance of a feature determines how much predictive power it contains the lower the variance is the less information contained in the features and less value it has in predicting the response variable.\n",
    "        Variance threshold is done by fining the variance of each feature and then dropping all of features below a certain variance threshold.\n",
    "        \n",
    "        \n",
    "  Feature selection WRAPPER approach:\n",
    "      Wrapping methodes compute models with a certain subset of features and evaluate the importance of each feature.Then they iterative and try a different subset of features untill the optial subset reached.\n",
    "      The most notable WRAPPER of feature selection are  1.FORWORD  SELECTION, 2.BACKWORD SELECTION AND 3.STEPWISE SELECTION \n",
    "      \n",
    "ADVANTAGES:\n",
    "1.The main benfit of feature selection is that it reduces overfitting by removing extraneous data\n",
    "2.Removing irrelevent information i.e improves the accuracy of the model prediction.\n",
    "3.It also reduces computational time involved to get model.\n",
    "\n",
    "DRAWBACKS:\n",
    "Large computational time for data with many features and , it tends to overfit the model when there is not large amount of data points .\n",
    "\n",
    "FORWARD SELECTION: \n",
    " It starts with Zero  features then for each individual all features , runs a model and dteremines the p-value associated with the t_test or F-test performed.\n",
    " It then selects the feature with lowest p-value and adds that to the working model , next it takes the first feature selected and runs models with a second feature added with lowest p-value, Then it takes the previously  two features and  run a model with third feature , it is repeated untill allfeatures that have significant p-value are added to model\n",
    " \n",
    " BACKWARD SELECTION:\n",
    "       Starts with all features contained in the dataset , it then runs a model and calculates p-value associated with t-test or f-test of the model for each feature , The feature which has largest significant p-value be renmoved from the model.\n",
    "       \n",
    "STEP WISE SELECTIOn:\n",
    "   It is a hybrid of forward and backward section. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4edbd3cc",
   "metadata": {},
   "source": [
    "\n",
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ee55166",
   "metadata": {},
   "source": [
    "Overall feature selection process.\n",
    "Feature selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data.\n",
    "  \n",
    "  When we have a 100+ features in a dataset that is ridiculous cannot to process normally which where feature selection methods come in handy,\n",
    "They allow you to reduce the number of features included in a model without sacrifising the predictive power.\n",
    "Features that are redundant or irrelevent can actually negatively impact your model performance , So it is necessary to remove them.\n",
    "There are three types of feature selection .\n",
    "\n",
    "1.WRAPPER METHODS(Forword, back word and stepwise).\n",
    "2.FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold)\n",
    "3.ENSEMBLE METHODS(Lasso,Ridge and Decision tree)\n",
    "FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold):\n",
    "    filter methods use a measure rather than error rate to determine whether the feature is useful rather than knowing a model , a subset of the features is selected through ranking them by a useful descriptive mesure.\n",
    "    ADVANTAGE: Benfits of filter methods That they have a very low computation time and will not overfit the data.\n",
    "    DRAW BACK: However One draw back is that they are blind to any interactions or correlation between features.This will need to taken into account seperately, some filter method are ANOVA,Pearson's correlation co-efficient and Variance Threshold.\n",
    "    \n",
    " 1. ANOVA(Analysis of variance):\n",
    "     The ANOVA test works with the variation within the treatments of a feature and also between the treatments.\n",
    "     These variance are important metrices for specified Filter method, Because we can determine whether a feature does a good job accounting for variation in the dependent variable.\n",
    "     An ANNOVA test has F-statistics is computed for each individual feature with the variation between  traetments in the numeric and the variation with in treatments in denominator.\n",
    "     \n",
    "  2.Pearson's correlation Coefficient:\n",
    "         Pearson correlation coefficient is a measure of the similarity of two features that ranges between -1 to 1.\n",
    "         A value close to 1 or -1 indicates that the features have a high correlation and may be relatedthe coefficients of high correlation vs low correlation depends on the range of correlation coefficients with in each dataset.\n",
    "         \n",
    "  3.VARIANCE THRESHOLDING: \n",
    "        The variance of a feature determines how much predictive power it contains the lower the variance is the less information contained in the features and less value it has in predicting the response variable.\n",
    "        Variance threshold is done by fining the variance of each feature and then dropping all of features below a certain variance threshold.\n",
    "        \n",
    "        \n",
    "  Feature selection WRAPPER approach:\n",
    "      Wrapping methodes compute models with a certain subset of features and evaluate the importance of each feature.Then they iterative and try a different subset of features untill the optial subset reached.\n",
    "      The most notable WRAPPER of feature selection are  1.FORWORD  SELECTION, 2.BACKWORD SELECTION AND 3.STEPWISE SELECTION \n",
    "      \n",
    "ADVANTAGES:\n",
    "1.The main benfit of feature selection is that it reduces overfitting by removing extraneous data\n",
    "2.Removing irrelevent information i.e improves the accuracy of the model prediction.\n",
    "3.It also reduces computational time involved to get model.\n",
    "\n",
    "DRAWBACKS:\n",
    "Large computational time for data with many features and , it tends to overfit the model when there is not large amount of data points .\n",
    "\n",
    "FORWARD SELECTION: \n",
    " It starts with Zero  features then for each individual all features , runs a model and dteremines the p-value associated with the t_test or F-test performed.\n",
    " It then selects the feature with lowest p-value and adds that to the working model , next it takes the first feature selected and runs models with a second feature added with lowest p-value, Then it takes the previously  two features and  run a model with third feature , it is repeated untill allfeatures that have significant p-value are added to model\n",
    " \n",
    " BACKWARD SELECTION:\n",
    "       Starts with all features contained in the dataset , it then runs a model and calculates p-value associated with t-test or f-test of the model for each feature , The feature which has largest significant p-value be renmoved from the model.\n",
    "       \n",
    "STEP WISE SELECTIOn:\n",
    "   It is a hybrid of forward and backward section. \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cd46b78",
   "metadata": {},
   "source": [
    "Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "\n",
    "\n",
    "Feature extraction refers to the process of transforming raw data into numerical features That can be processed while preserving the information in the original dataset.It yields better result than applying ML directly to the raw data.\n",
    "Feature extraction can be accomplished manually or automatically.\n",
    "    \n",
    "    Manual feature extraction requires identifying and describing the features that are relevant for a given problem and implementing a way to extract those features.\n",
    "    Automated feature extraction uses specialized algorithm or deep networks to extract features automatically from signals or images with out need for human invention.\n",
    "    \n",
    "    The most common linear methods for feature extraction are PCA   and LDA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d34aefd",
   "metadata": {},
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a1fc6e3",
   "metadata": {},
   "source": [
    "Environment setup: import packages and real data\n",
    "Language detection:Understand which natural language data is.\n",
    "Text preprocessing:text cleaning and transformation.\n",
    "Length analysis: Measured with different metrics.\n",
    "Sentiment analysis: Determine whether a text is positivr or negative.\n",
    "Word Vectors:Transforms a word into numbers .\n",
    "TOPIC MODELING: Extract the main topics from corpus."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c40014fd",
   "metadata": {},
   "source": [
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3375b89b",
   "metadata": {},
   "source": [
    "Cosine similarity is used as a  metric in different ML algorithms like KNN for determining the distance between the neighbours, In recommendation system it is used to recommend movies with the same similarity of text in the document.\n",
    "Cosine similarity is the cosine of angle between two vectors and it is used as a distance elevation metric between two points in the plane, The cosine similarity measures operates entirely on the cosine principles with the increase in distance the similarity of data points reduces.\n",
    "  Cosine similarity in textual data is used to compare the similarity  between two text documents or to kenized texts  \n",
    "  \n",
    "  \n",
    "  sim(x,y) = x*y/(||x|| ||y||)\n",
    "  \n",
    "  here 'x'is a vector x=(x1,x2,x3,x4,...xn), \n",
    "  then ||x|| is given as sqrt(x1^2+x2^2+x3^2+....+xn^2)  euclidean norm of vector \"x\" , and ||y|| euclidean norm of vector 'y'.\n",
    "  \n",
    "  \n",
    "  Cosine of '0' means that two vectors are at 90 degrees each other and have  no match \n",
    "  \n",
    "  Here given x=(2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
    "             y=(2, 1, 0, 0, 3, 2, 1, 3, 1) \n",
    "             \n",
    "             \n",
    "             =4+3+0+0+0+6+6+3+0+1/((4+9+4+0+4+9+9+0+1)(4+1+0+0+9+4+1+9+1)\n",
    "             =23/40+29\n",
    "             =23/69\n",
    "             =0.33\n",
    "             \n",
    "     cosine similarity of given x, y is 0.33\n",
    "             "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e718112",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73fe4fdf",
   "metadata": {},
   "source": [
    "What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "\n",
    "In order to calculate the hamming distance between twostrings and we perform their XOR operation  and then count the total number of 1's in the resultant.\n",
    "i.e  Hamming distance is the number of values that are different between two vector \n",
    "\n",
    "Given two vectors   10001011  and 11001111,\n",
    "   from these two  perorming XOR operation and summing resultant 1's sum is 2."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd8729ba",
   "metadata": {},
   "source": [
    "Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "In some casesif we use SMC(simple matching coefficient) , we will get scores which will be biased by attributes which rarely created red flag (all values comes equal ) in these case we use jaccard index,\n",
    "      \n",
    "      \n",
    "      Jaccard index of each pair is calculated as J=M11/(M01+M10+M11)\n",
    "          \n",
    "          where 'M11' is number of attributes where both attributes have Red(ones) falgs\n",
    "               'M01,M10' is the number of attributes has red(ones) flag and other has green(zero) flag \n",
    "               \n",
    "          Here we have   A,B,C tarributes then caculating jaccard indes for eac pair \n",
    "          \n",
    "          A=(1, 1, 0, 0, 1, 0, 1, 1)\n",
    "          B= (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "          C=(1, 0, 0, 1, 1, 0, 0, 1)\n",
    "          \n",
    "       Jaccard index of AB= 4/(4+1+1)=4/6=0.66\n",
    "       Jaccard index of AC=3/(3+2+0)=3/5=0.60\n",
    "       Jaccard index of BC =2/(2+3+2)=2/7=0.28\n",
    "       \n",
    "   Jaccard index also called as IOU(intersection over union) metric which is used while doing semantic segmentation an image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "950a9655",
   "metadata": {},
   "source": [
    "8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06648734",
   "metadata": {},
   "source": [
    "High dimeensional dat refers to a dataset in which the number of features (p) is larger than the number of observations N, often written as P>>N.\n",
    "  \n",
    "      \n",
    "      For example a dataset that has p=6 features and only N=1 observations would be considered as high dimensional data.\n",
    "      When the number of features in a datset exceeds the number of observations , we will never have a deterministic answer.It becomes impossible to find a model that can describe relationship between predictor variable and response variable because we dont have enough observations to train the model.\n",
    "      \n",
    "      Some real examples of high dimensional dataset are \n",
    "      \n",
    "      HEALTH CARE: high dimensional datset is common in helthcare datasets where the number of features for a given individual can be massive \n",
    "      \n",
    "     There are two two most common ways to handle high dimensional data\n",
    "    1.Choose to include few features\n",
    "           simply include feature in the datset there are several ways to to decide which feature to drop  from a dataset\n",
    "           \n",
    "           1.Drop features with many missing values.\n",
    "           2.drop features with low variance\n",
    "           3.Drop features with low correlation with the response variable.\n",
    "    \n",
    "    2.Use a regularization method \n",
    "       Without dropping features fro the dataset is to use a regularization technique such as PCA, Ridge regression  and lasso regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1559f89e",
   "metadata": {},
   "source": [
    "9. Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59509a11",
   "metadata": {},
   "source": [
    "Use of vectors\"\n",
    "  Vectors are foundational element in algebra.Vectors are used through out the field of ML in the description of algorithms and process such as the target variable .\n",
    "  When training a algorithm , The vector can also be thought of as a line from the origin of the vector space with a directional magnitude.\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "13c7af27",
   "metadata": {},
   "source": [
    " Embedded technique:\n",
    "    Enbeddings make it easier to do ML on large inputs like sparse vectors representing words , Ideally embedding capture some of the semantics of the input by placing semantically similar inputs close togrther in the embedding space . An embedding can be learnded and reused across models.\n",
    "     The word embeddedtechniques are used to represent words mathematically , exaple: One hot encoding, TF-IdF, word2vec ..etc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6aed52a6",
   "metadata": {},
   "source": [
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff5e6020",
   "metadata": {},
   "source": [
    " Sequential backward exclusion vs. sequential forward selection:\n",
    " FORWARD SELECTION: \n",
    " It starts with Zero  features then for each individual all features , runs a model and dteremines the p-value associated with the t_test or F-test performed.\n",
    " It then selects the feature with lowest p-value and adds that to the working model , next it takes the first feature selected and runs models with a second feature added with lowest p-value, Then it takes the previously  two features and  run a model with third feature , it is repeated untill allfeatures that have significant p-value are added to model\n",
    " BACKWARD SELECTION:\n",
    "       Starts with all features contained in the dataset , it then runs a model and calculates p-value associated with t-test or f-test of the model for each feature , The feature which has largest significant p-value be renmoved from the model.\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "97e440b9",
   "metadata": {},
   "source": [
    "  Function selection methods: filter vs. wrapper\n",
    "  \n",
    "  Feature selection WRAPPER approach:\n",
    "      Wrapping methodes compute models with a certain subset of features and evaluate the importance of each feature.Then they iterative and try a different subset of features untill the optial subset reached.\n",
    "      The most notable WRAPPER of feature selection are  1.FORWORD  SELECTION, 2.BACKWORD SELECTION AND 3.STEPWISE SELECTION \n",
    "      \n",
    "      FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold):\n",
    "    filter methods use a measure rather than error rate to determine whether the feature is useful rather than knowing a model , a subset of the features is selected through ranking them by a useful descriptive mesure.\n",
    "    ADVANTAGE: Benfits of filter methods That they have a very low computation time and will not overfit the data.\n",
    "    DRAW BACK: However One draw back is that they are blind to any interactions or correlation between features.This will need to taken into account seperately, some filter method are ANOVA,Pearson's correlation co-efficient and Variance Threshold."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f55ea05",
   "metadata": {},
   "source": [
    "SMC vs. Jaccard coefficient\n",
    "\n",
    "\n",
    "SMC: \n",
    "   SMC means simple matching coefficient .This is very simple and intutive approch to building a recommendation systems.\n",
    "   SMC is useful in the cases where vectors have binary attributes and both class carry equal infomation.\n",
    "   SMC formula is given as \n",
    "       SMC= (M00+M11)/(M00+M11+M10+M01)\n",
    "       \n",
    "       lets take we hve two datasets  A and B ,\n",
    "       \n",
    "       then  where M11 ---> both A and B have the same (red flag)\n",
    "             where M00 ----> both A and B have same (Green flag)\n",
    "                   M10    ---> A is red flag and B is Green flag\n",
    "                   M01   ----> A is green flag and B is red flag.\n",
    "                   \n",
    "                   \n",
    " In some casesif we use SMC(simple matching coefficient) , we will get scores which will be biased by attributes which rarely created red flag (all values comes equal ) in these case we use jaccard index,\n",
    "      \n",
    "      \n",
    "      Jaccard index of each pair is calculated as J=M11/(M01+M10+M11)\n",
    "          \n",
    "          where 'M11' is number of attributes where both attributes have Red(ones) falgs\n",
    "               'M01,M10' is the number of attributes has red(ones) flag and other has green(zero) flag \n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
