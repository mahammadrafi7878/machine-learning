{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e240774d",
   "metadata": {},
   "source": [
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b97f9e0d",
   "metadata": {},
   "source": [
    "The depth of a well balanced binary tree containing 'm' leaves is equal to log2 (m)3 randed up.\n",
    "A binary decision tree(one that makes only binary decisions,as is the case of all tress in scikit learn) will end up more or less well balalnced at the en of training , with one leaf per training instance,.\n",
    "If it is trained without restictions thus if the training set contains one million instances , then the decision tree will have a depth of log2 (106) approximately equal to 20(actually a bit more since the tree will generally not be perfectly well balanced.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ab532d5",
   "metadata": {},
   "source": [
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f0c4729",
   "metadata": {},
   "source": [
    "A node's gini impurity is generally lower than its parents's.\n",
    "This is ensured by the CART training algorithm's cost function, which splits a node in a way that minimizes the weighted sum of its children gini impurities .\n",
    "However if one child is smaller than the other , it is possible for it to have a higher gini impurity than its parent, as long as this increase is more than compensated for by a decrease of the other child's impuritiey.\n",
    "For example consider a node containing four instances of clqass A and one of class B , it's gini impurity is \n",
    "1-1/5^2 - 4/5^2  =0.32.\n",
    "Now suppose the dataset is one dimensional and the instances are lined up in the following order A,B,A,A,A you can verify that the algorithm will split this node after second instance producing one child node with instances A,B and the other child node with instances A,A,A.\n",
    "The first child node gini impurity is 1-1/2^2 - 1/2^2 =0.5 which is higher than the its parent .\n",
    "This is compensated for by gini impurity is 25(0.5)+35(0) =0.2 , which is lower than its parent's gini impurity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "63afb3ee",
   "metadata": {},
   "source": [
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb92a854",
   "metadata": {},
   "source": [
    "If a decision tree is overfitting the training set , it may be good a idea to decrease the maximum depth, since this will constrin model and regularizes it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "de57b48c",
   "metadata": {},
   "source": [
    "4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39ba30e4",
   "metadata": {},
   "source": [
    "Decision tree dont care whether or not the training dataset is scaled or centered.\n",
    "That's one of the nice things about them , so if a decision tree underfits the training set, scaling the input features will just be wqaste of time."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ed80a6c",
   "metadata": {},
   "source": [
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cea5554",
   "metadata": {},
   "source": [
    "The computational complexity of a training decision tree is  O(n*m(log(m))).\n",
    "So if you multiply the training set size by 10,The training time will be multiplied by \n",
    "K=(n*10m(log(10m)))/(n*m*log(m))) =10*log(10m)/log(m).\n",
    "if m=106, then K approximately =-11.7,\n",
    "So you can expect the training time to be roughly 11.7 hours."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28516632",
   "metadata": {},
   "source": [
    "6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd8433b8",
   "metadata": {},
   "source": [
    "Presorting the training set speeds up training only if the dataset smaller than a few thousand instances.\n",
    "If it contains 1,00,00 instances , setting presort+True will considerably slow down training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae129b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4f626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "49ce793a",
   "metadata": {},
   "source": [
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "316d795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e0f53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=make_moons(n_samples=10000,noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3a582b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.533138  ,  1.03964571],\n",
       "        [-1.4279957 ,  0.126611  ],\n",
       "        [ 1.5764244 ,  0.44264574],\n",
       "        ...,\n",
       "        [ 0.11587175,  0.83105812],\n",
       "        [ 0.20775445,  1.10953354],\n",
       "        [ 0.3981268 , -0.143565  ]]),\n",
       " array([0, 0, 0, ..., 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba0338dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6eb73b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c27e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de96f79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd8500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\" : [\"best\", \"random\"],\n",
    "    \"min_samples_leaf\":range(10),\n",
    "    \"max_depth\":range(10),\n",
    "    \"min_samples_split\":range(10),\n",
    "    \"max_leaf_nodes\":range(10)\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1f3c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "144480 fits failed out of a total of 300000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 238, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: max_depth == 0, must be >= 1.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "27000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 247, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_leaf == 0, must be >= 1.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 0, must be >= 2.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "19440 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 350, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: max_leaf_nodes == 0, must be >= 2.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "19440 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 350, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: max_leaf_nodes == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\WONDER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [   nan    nan    nan ... 0.7987 0.859  0.8029]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 2,\n",
       " 'max_leaf_nodes': 4,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model=tree.DecisionTreeClassifier()\n",
    "grid_search=GridSearchCV(model,params)\n",
    "grid_search.fit(x,y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad362ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc=tree.DecisionTreeClassifier(criterion= 'gini',max_depth= 2,max_leaf_nodes= 4,min_samples_leaf= 1,min_samples_split= 2,splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5df4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dtc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3708380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8598666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dtc.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca8c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=model_dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537268e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "887070f0",
   "metadata": {},
   "source": [
    "8. Follow these steps to grow a forest:\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplitLearn's class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a256c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.datasets import make_moons\n",
    "result=ShuffleSplit(n_splits=1000,train_size=0.0125,random_state=77)\n",
    "result.get_n_splits(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a29aa228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=1000, random_state=77, test_size=None,\n",
      "       train_size=0.0125)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4125b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subsets=dict()\n",
    "y_subsets=dict()\n",
    "for i, indexes in enumerate(result.split(x_train)):\n",
    "    x_subsets[i]=x_train[indexes[0],:]\n",
    "    y_subsets[i]=y_train[indexes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6004d9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_subsets[0].shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbb72b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_subsets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acfdd1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_subsets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7635e6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_subsets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce67fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "model_1=tree.DecisionTreeClassifier(criterion= 'gini',max_depth= 2,max_leaf_nodes= 4,min_samples_leaf= 1,min_samples_split= 2,splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09b6bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "scores=[]\n",
    "for i in range(0,1000):\n",
    "    model_1.fit(x_subsets[i],y_subsets[i])\n",
    "    y_pred=model_1.predict(x_test)\n",
    "    scores.append(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8020c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7864, 0.7748, 0.8152, 0.8092, 0.8136, 0.8328, 0.8296, 0.8392, 0.8412, 0.812, 0.8312, 0.8376, 0.822, 0.84, 0.8436, 0.8392, 0.8264, 0.8452, 0.812, 0.8052, 0.8416, 0.8148, 0.8248, 0.7904, 0.824, 0.8372, 0.8432, 0.8444, 0.7824, 0.8196, 0.8436, 0.8304, 0.8288, 0.8116, 0.8212, 0.84, 0.8348, 0.842, 0.8456, 0.8176, 0.8324, 0.8048, 0.7792, 0.8412, 0.842, 0.8404, 0.7424, 0.8348, 0.8428, 0.8464, 0.8416, 0.846, 0.8304, 0.84, 0.8312, 0.816, 0.8408, 0.8168, 0.8228, 0.6732, 0.8456, 0.8004, 0.674, 0.8008, 0.8396, 0.8024, 0.8304, 0.7728, 0.8228, 0.8128, 0.8424, 0.8376, 0.7956, 0.8012, 0.6824, 0.664, 0.8172, 0.82, 0.822, 0.8352, 0.7916, 0.842, 0.8376, 0.818, 0.818, 0.844, 0.8428, 0.8188, 0.8416, 0.7788, 0.8316, 0.7784, 0.8032, 0.824, 0.8216, 0.8464, 0.7124, 0.8336, 0.8096, 0.8348, 0.81, 0.8464, 0.7928, 0.8256, 0.8396, 0.8332, 0.8044, 0.8404, 0.8024, 0.8464, 0.8424, 0.8432, 0.8392, 0.8472, 0.8012, 0.8372, 0.8212, 0.8144, 0.808, 0.842, 0.8016, 0.846, 0.7656, 0.838, 0.7756, 0.714, 0.7724, 0.8432, 0.8372, 0.8448, 0.8404, 0.8348, 0.7976, 0.8404, 0.8292, 0.8356, 0.8008, 0.8356, 0.7688, 0.7548, 0.8356, 0.7752, 0.8332, 0.8476, 0.8356, 0.8364, 0.8136, 0.826, 0.8444, 0.8296, 0.8028, 0.834, 0.7964, 0.7152, 0.836, 0.816, 0.836, 0.8176, 0.7996, 0.7896, 0.844, 0.8432, 0.778, 0.8116, 0.8404, 0.8304, 0.8176, 0.826, 0.8244, 0.832, 0.7432, 0.8236, 0.828, 0.834, 0.8064, 0.7756, 0.8364, 0.8456, 0.7984, 0.8116, 0.846, 0.7832, 0.8276, 0.8096, 0.82, 0.8408, 0.8376, 0.8336, 0.8304, 0.8476, 0.8416, 0.8448, 0.84, 0.824, 0.7616, 0.8324, 0.7804, 0.7764, 0.8444, 0.7864, 0.8432, 0.7848, 0.84, 0.776, 0.8116, 0.814, 0.76, 0.8196, 0.81, 0.8348, 0.7508, 0.8336, 0.8448, 0.81, 0.8164, 0.7104, 0.8448, 0.8264, 0.806, 0.8152, 0.814, 0.8408, 0.8076, 0.8424, 0.7284, 0.8472, 0.812, 0.818, 0.8448, 0.8376, 0.8452, 0.8352, 0.8116, 0.8168, 0.8464, 0.8364, 0.8392, 0.8272, 0.8212, 0.728, 0.8388, 0.8312, 0.8148, 0.774, 0.8388, 0.8276, 0.8444, 0.7956, 0.8364, 0.8072, 0.8212, 0.816, 0.7236, 0.844, 0.7624, 0.8168, 0.83, 0.7988, 0.84, 0.8444, 0.8332, 0.8444, 0.7632, 0.8212, 0.7732, 0.8436, 0.8356, 0.7604, 0.8364, 0.8064, 0.8296, 0.8016, 0.7612, 0.8, 0.8052, 0.8396, 0.78, 0.8112, 0.7976, 0.8444, 0.8352, 0.8456, 0.8204, 0.806, 0.8252, 0.8152, 0.8256, 0.8484, 0.8212, 0.8412, 0.8336, 0.708, 0.8456, 0.6756, 0.8412, 0.7956, 0.8424, 0.8368, 0.8408, 0.836, 0.8412, 0.8484, 0.8152, 0.8304, 0.7896, 0.8072, 0.8432, 0.8448, 0.8352, 0.8408, 0.8384, 0.8052, 0.8088, 0.8352, 0.8464, 0.8432, 0.8292, 0.8252, 0.8024, 0.8432, 0.8108, 0.83, 0.7148, 0.824, 0.8288, 0.8396, 0.8428, 0.8336, 0.8412, 0.842, 0.8364, 0.8468, 0.8412, 0.8208, 0.8396, 0.8476, 0.8176, 0.8348, 0.8104, 0.8412, 0.8384, 0.7556, 0.844, 0.8448, 0.838, 0.8364, 0.8256, 0.8412, 0.8292, 0.7788, 0.8212, 0.8252, 0.8384, 0.7344, 0.818, 0.8348, 0.8296, 0.8368, 0.7936, 0.8, 0.8484, 0.8024, 0.832, 0.8064, 0.8396, 0.8468, 0.836, 0.8428, 0.7568, 0.8368, 0.8168, 0.818, 0.8444, 0.7596, 0.8476, 0.8152, 0.806, 0.8368, 0.842, 0.842, 0.818, 0.8092, 0.8444, 0.8404, 0.8504, 0.7952, 0.8132, 0.8416, 0.84, 0.8016, 0.8436, 0.8228, 0.8028, 0.8084, 0.8164, 0.8152, 0.8116, 0.8332, 0.8452, 0.652, 0.8384, 0.832, 0.8444, 0.846, 0.678, 0.8148, 0.8292, 0.8208, 0.7956, 0.6652, 0.8164, 0.8156, 0.7804, 0.838, 0.8396, 0.8128, 0.8384, 0.842, 0.8168, 0.846, 0.84, 0.8, 0.8352, 0.8252, 0.8364, 0.8316, 0.8164, 0.7928, 0.8124, 0.848, 0.8424, 0.8144, 0.8328, 0.8124, 0.8176, 0.8456, 0.8484, 0.806, 0.8492, 0.838, 0.8276, 0.7968, 0.8048, 0.8104, 0.8388, 0.8468, 0.8436, 0.7796, 0.8436, 0.7928, 0.8232, 0.8128, 0.8404, 0.848, 0.8036, 0.8376, 0.8324, 0.8444, 0.6776, 0.7868, 0.806, 0.8396, 0.7956, 0.7372, 0.7968, 0.83, 0.8188, 0.796, 0.8444, 0.8312, 0.8204, 0.8012, 0.8424, 0.8232, 0.8368, 0.8372, 0.8452, 0.846, 0.8448, 0.7896, 0.8132, 0.8424, 0.8448, 0.83, 0.8356, 0.8096, 0.8476, 0.8244, 0.7772, 0.8348, 0.8292, 0.8412, 0.7692, 0.8252, 0.7628, 0.7576, 0.7916, 0.8352, 0.8364, 0.8392, 0.8072, 0.8312, 0.8428, 0.7904, 0.8436, 0.7984, 0.796, 0.8436, 0.806, 0.8136, 0.8364, 0.8332, 0.8448, 0.8152, 0.8108, 0.844, 0.8128, 0.8432, 0.8408, 0.8248, 0.8408, 0.8464, 0.8488, 0.812, 0.7628, 0.8356, 0.8412, 0.8412, 0.8064, 0.7924, 0.8184, 0.7984, 0.8404, 0.8056, 0.7224, 0.7944, 0.8104, 0.8284, 0.816, 0.8064, 0.804, 0.8452, 0.7772, 0.7608, 0.7908, 0.8212, 0.8136, 0.814, 0.7976, 0.8152, 0.8388, 0.8276, 0.8068, 0.8308, 0.7988, 0.8464, 0.8388, 0.838, 0.8436, 0.782, 0.8112, 0.7784, 0.8036, 0.8468, 0.8464, 0.7956, 0.8464, 0.8348, 0.7932, 0.6628, 0.8468, 0.8444, 0.796, 0.8456, 0.7708, 0.8392, 0.8404, 0.832, 0.7964, 0.8028, 0.8144, 0.7988, 0.7972, 0.8032, 0.838, 0.7836, 0.8368, 0.8396, 0.848, 0.838, 0.7844, 0.848, 0.8396, 0.8416, 0.7784, 0.8332, 0.7256, 0.8072, 0.8444, 0.8344, 0.8396, 0.8396, 0.784, 0.802, 0.788, 0.824, 0.8216, 0.818, 0.8344, 0.6664, 0.8352, 0.8424, 0.8356, 0.7916, 0.7888, 0.7792, 0.8404, 0.836, 0.7648, 0.8184, 0.826, 0.8132, 0.836, 0.7852, 0.8444, 0.848, 0.8032, 0.8392, 0.8264, 0.8472, 0.8004, 0.7672, 0.7932, 0.8368, 0.8152, 0.818, 0.838, 0.8276, 0.8072, 0.8284, 0.8144, 0.836, 0.8472, 0.838, 0.832, 0.8424, 0.8452, 0.8444, 0.8448, 0.8256, 0.8192, 0.836, 0.6608, 0.768, 0.8416, 0.8396, 0.8388, 0.8472, 0.8208, 0.8476, 0.8368, 0.8232, 0.8408, 0.8084, 0.8444, 0.8156, 0.7968, 0.8212, 0.8408, 0.8352, 0.8044, 0.8416, 0.7996, 0.8036, 0.828, 0.8148, 0.7816, 0.7856, 0.826, 0.83, 0.8096, 0.786, 0.8212, 0.7712, 0.8284, 0.8064, 0.8228, 0.8192, 0.838, 0.8332, 0.8168, 0.8124, 0.8092, 0.8164, 0.816, 0.8348, 0.7936, 0.8136, 0.7948, 0.8428, 0.8368, 0.824, 0.7548, 0.8364, 0.8412, 0.8448, 0.8168, 0.8084, 0.7812, 0.8208, 0.8384, 0.83, 0.7496, 0.8392, 0.8344, 0.8436, 0.7936, 0.7868, 0.7772, 0.8264, 0.8188, 0.8404, 0.8396, 0.8392, 0.8312, 0.8112, 0.768, 0.814, 0.7988, 0.842, 0.8472, 0.8252, 0.8324, 0.8464, 0.8076, 0.8436, 0.8232, 0.7984, 0.84, 0.7976, 0.8184, 0.818, 0.8324, 0.6764, 0.8372, 0.8372, 0.7248, 0.8228, 0.8348, 0.7848, 0.8096, 0.81, 0.79, 0.83, 0.8352, 0.8188, 0.8444, 0.8096, 0.8184, 0.844, 0.7892, 0.8352, 0.7724, 0.8408, 0.8428, 0.7956, 0.838, 0.8104, 0.8376, 0.838, 0.8112, 0.7804, 0.8168, 0.8464, 0.8416, 0.8376, 0.7632, 0.8412, 0.8356, 0.7964, 0.8044, 0.8148, 0.6708, 0.8032, 0.8068, 0.8336, 0.8136, 0.8472, 0.8464, 0.7936, 0.7952, 0.8192, 0.8084, 0.8296, 0.8424, 0.8444, 0.814, 0.786, 0.8432, 0.8392, 0.844, 0.826, 0.8224, 0.844, 0.8184, 0.79, 0.7448, 0.8324, 0.8056, 0.8012, 0.8116, 0.8012, 0.7852, 0.8044, 0.7604, 0.8468, 0.842, 0.8212, 0.808, 0.8416, 0.7768, 0.8248, 0.8284, 0.7412, 0.8408, 0.8428, 0.8428, 0.7396, 0.7768, 0.846, 0.7704, 0.7924, 0.8316, 0.842, 0.7988, 0.8232, 0.8488, 0.7736, 0.8204, 0.8276, 0.8428, 0.8436, 0.814, 0.7732, 0.8336, 0.8432, 0.8224, 0.826, 0.832, 0.674, 0.8224, 0.8456, 0.7144, 0.8136, 0.8212, 0.8372, 0.822, 0.8452, 0.7924, 0.8092, 0.83, 0.8384, 0.8196, 0.7504, 0.8144, 0.796, 0.712, 0.6788, 0.8412, 0.8456, 0.8224, 0.8232, 0.838, 0.8396, 0.8036, 0.7444, 0.7676, 0.8464, 0.8308, 0.8392, 0.8408, 0.8444, 0.8444, 0.7748, 0.834, 0.8012, 0.8264, 0.8212, 0.8208, 0.84, 0.8348, 0.8436, 0.7892, 0.8448, 0.8152, 0.842, 0.8312, 0.8164, 0.8388, 0.844, 0.8412, 0.8272, 0.8376, 0.8148, 0.8396, 0.8388, 0.814, 0.8208, 0.8384, 0.8076, 0.7356, 0.834, 0.8156, 0.8376, 0.7928, 0.7912, 0.8448, 0.7756, 0.8196, 0.8232, 0.7696, 0.8396, 0.8388, 0.8412, 0.8452, 0.8312, 0.8084, 0.8196, 0.8412, 0.8376, 0.8108, 0.7764, 0.8456, 0.7856, 0.8448, 0.762, 0.8444, 0.8424, 0.8408, 0.8408, 0.8416, 0.828, 0.8444, 0.8048, 0.8396, 0.7928, 0.8456, 0.8008, 0.832, 0.7712, 0.832, 0.8332, 0.764, 0.8444, 0.8028, 0.8384, 0.8104, 0.8216, 0.7764, 0.7976, 0.7932, 0.8352, 0.8256, 0.8152, 0.8364, 0.8396, 0.836, 0.8424, 0.788, 0.81, 0.8412, 0.8392, 0.784, 0.8448, 0.8244, 0.842, 0.7988, 0.812, 0.834, 0.7764, 0.8452, 0.8416, 0.792, 0.85, 0.772, 0.8116, 0.8364, 0.7836, 0.7672, 0.8304, 0.8216, 0.8108, 0.8088, 0.7512, 0.848, 0.8036, 0.8484, 0.7688, 0.8412, 0.8324]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e68f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 85.04%.\n",
      "Min accuracy: 65.20%.\n"
     ]
    }
   ],
   "source": [
    "print(\"Max accuracy: {:.2f}%.\".format(max(scores) * 100))\n",
    "print(\"Min accuracy: {:.2f}%.\".format(min(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87940aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building random forest\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "y_pred1=[]\n",
    "for instance in range(0,x_test.shape[0]):\n",
    "    predictions=[]\n",
    "    for i in range(0,100):\n",
    "        model_1.fit(x_subsets[i],y_subsets[i])\n",
    "        predictions.append(model_1.predict(x_test[i].reshape(-1,2)))\n",
    "    y_pred1.append(stats.mode(np.array(predictions),axis=None)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62863cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d2223c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 83.24%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730f09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b501b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10118b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708e6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
