{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3d663da1",
   "metadata": {},
   "source": [
    "1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2732e114",
   "metadata": {},
   "source": [
    "If you have trained five different models and they all achieves precision, you can try combining them into a voting ensemble, which will often give you even better results.\n",
    "It works better if the model are very different (e.g SVC,DT,LOGI).\n",
    "It is even better they are trained on a different training instances (i.e the whole point of bagging and pasting ensembles), but if not it will still work as long as the models are very different."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f312d0d3",
   "metadata": {},
   "source": [
    "2. What's the difference between hard voting classifiers and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "240c3c0a",
   "metadata": {},
   "source": [
    "Hard voting classifiers just counts the votes of each classifier in the ensemble and picks the class that get's thge most votes.\n",
    "A soft voting classifier computes the average estimated class probability for each class and picks the class with the highest probability.This gives high confidence votes more weighted and often performs better, But it works only if every classifier is able to estimate class probabilities."
   ]
  },
  {
   "cell_type": "raw",
   "id": "747a6cf4",
   "metadata": {},
   "source": [
    "3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options."
   ]
  },
  {
   "cell_type": "raw",
   "id": "20497ff4",
   "metadata": {},
   "source": [
    "It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers , since each predictor in the ensemble is independent of others .\n",
    "the same goes for pasting ensembles and Random Forest for the same reason.\n",
    "How ever each predictor in boosting ensemble is built based on the previous predictor, so training is necessarly sequential and you will not gain any thing by distributing training across multiple servers.\n",
    "Regarding stacking ensembles all the predictions in a given layer are independent of each other so, they can be trained in parallel on multiple servers.\n",
    "How ever , The predictors in one layer can only be trained after the predictors in previous layer have allbeen trained."
   ]
  },
  {
   "cell_type": "raw",
   "id": "79badde4",
   "metadata": {},
   "source": [
    "4. What is the advantage of evaluating out of the bag?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "746cdc8b",
   "metadata": {},
   "source": [
    "With OOB(out of bag ) evaluation , each predictor in bagging ensemble is evaluated using instances that is way not trained on(they were held out).\n",
    "This makes it is possible to have a fairely unbiased evaluation of the ensemble  with out the need for an additional validation set.\n",
    "Thus you have more instances available for training and your ensemble can perform slightly better."
   ]
  },
  {
   "cell_type": "raw",
   "id": "596a2e07",
   "metadata": {},
   "source": [
    "5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "479056c4",
   "metadata": {},
   "source": [
    "When you are growing a tree in Random Forest , Only a random subset of  features is considered for spliting at each node.\n",
    "This tree as well for extra trees , But they go one step further  rather than searching for the best possible thresholds for each feature, This extra randomness acts like a form regularization.\n",
    "If a random forest overfits the training data , extra tree might better morever since extra trees does not search for best possible thrsholds , They are much faster to train the random forests.\n",
    "However they are neither faster nor slower than random forests when making predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "75c15d70",
   "metadata": {},
   "source": [
    "\n",
    "6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f774134",
   "metadata": {},
   "source": [
    "If your ada boost ensemble underfits the training data , You can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator .\n",
    "You may also try  slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "db2a238d",
   "metadata": {},
   "source": [
    "7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b7f6c02",
   "metadata": {},
   "source": [
    "If you are gradient boosting ensemble overfits the training set , you should try decreasing the learning rate.\n",
    "You could also use early stopping to find the right number of parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
