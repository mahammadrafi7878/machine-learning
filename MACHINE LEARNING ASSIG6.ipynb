{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bbfd1c4",
   "metadata": {},
   "source": [
    "1. In the sense of machine learning, what is a model? What is the best way to train a model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2622e95c",
   "metadata": {},
   "source": [
    "A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data,providing it an algorithm that it can used to reason over and learn from the data.\n",
    "    Once you have trained the model , you can use it to reason overdata that it hasnot seen befor  and make predictions about those data.\n",
    "    The best way to train a model is training data with all machine learning models with differnt parametrs and take a model which gives best accuracy. In machine learning there is no best algorithm based on data and nature of the problem we need to train all ML models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4fc5dc",
   "metadata": {},
   "source": [
    "2. In the sense of machine learning, explain the \"No Free Lunch\" theorem.\t"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72a65487",
   "metadata": {},
   "source": [
    "The \"NO FREE LUNCH\" (NFL) theorem for supervised machine learning is a theorem that essentially implies that \"No single machine learning algorithm is universally the best performing algorithm for all problems\".\n",
    "WOLPERT introduced NFL theorem , this theorem ststes that given a noise free dataset, for any two machine learning algorithm A and B , The average performance of A and B will be the same acros  all possible problem instances drawn from a uniform probabilty distribution.\n",
    "    This is beacause of every ML algorithm makes a prior assumptions about the relationship between the features and target variable for ML problem.\n",
    "    A algorithm may perform very well for one problem, but that gives us no reason to believe it will do best as well ondifferent problem , where the same assumption may not work."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4db11c2b",
   "metadata": {},
   "source": [
    "3. Describe the K-fold cross-validation mechanism in detail."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d1f2c59",
   "metadata": {},
   "source": [
    "Cross Validation sometimes called rotation estimation or out of sample testing, is any of various similar model validation techniques for accessing how the results of statistical analysis will generate to an independent dataset.\n",
    "\n",
    "Cross Validation is resampling method that uses a different portions of the data to test and train a model on different iterations.\n",
    "   In k-fold cross validation , The original sample is randomply paritioned into 'k' equal subsamples.  Of the k-samples a single subsample is retained as validation data for testing the model and remaining k-1 subsamples are used as training dataset.\n",
    "     The cross validation process is repeated 'k' times, with each of the k-sub samples used exactly once as the validation data . The 'k' results can be averaged to produce a single estimation .\n",
    "     The advantage of this method over repeated random subsampling is that all observations are used for validation exactly once .  1.-fold cross validation is generally used ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5acf266",
   "metadata": {},
   "source": [
    "4. Describe the bootstrap sampling method. What is the aim of it?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c7de5c6",
   "metadata": {},
   "source": [
    "In statistics and ML bootstrap is resampling techniques that involves repeatedly drawing samples form our source data with replacement, often to estimate  a population pareameter. By replacement means that the same data point may be included in our resampled dataset multilple times.\n",
    "SAMPLING:   with respect to statistics sampling is the process of selecting a subset of items from a vast collection of items to estimate a certaqin chances characteristics of the entire data or population, Sampling with repalce ment a datapoint in a drawn sample can reappear in future drawn sample as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "04c055d3",
   "metadata": {},
   "source": [
    "5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e80e1e3",
   "metadata": {},
   "source": [
    "Kappa Score is a interesting metric like f1score, precision and recall metrices for classification model.\n",
    "Kappa score is origin in the field of psychology.It later appropriated by themachine learning community to measure classification performance ,also known as cohens Kappa coefficent.\n",
    "The kappa score measures the degree of agreement between two evaluators also known as inter rater reliabilities.\n",
    "It is pretty similar to confusion matrix , lets say where we built a classifier detects the givenbelow   \n",
    "\n",
    "                   True actual  professor A\n",
    "                    accept     waiting    reject \n",
    "predicted                                                 eg. collage education seats ratings  given by two professor \n",
    "professor B     accept  4          6       3                      for 25 students, whether student can get seat or not? \n",
    "              waiting   1          2       0\n",
    "               reject   1          2       6\n",
    "         \n",
    "       \n",
    "     To use kappa for classification , we can simply think of each rating as a class and the ratings are true actual values \n",
    "     In this case the kappa score  measures the degree of agreement between the true values and predicted values . Which we use as a classification performance.\n",
    "     \n",
    "     \n",
    "     To compute kappa score for above problem,\n",
    "  A naive way to measure agreement would be to look at the portion of ratings that are in agreement between the variables, lets call this number of agree easy of \"sum of all diagonal and divide by total students\"\n",
    "  \n",
    "  In this case agree = sum of diagonal elements divide by total ==> 12/25 =0.48\n",
    "  \n",
    "  \n",
    "   This is perfect metric for an agreement , here our metric is not very sophisticated , since it does not consider agreements  can also happen by chance.\n",
    "   The kappa score consider how much better the agreements are over and beyond chaqnce agreements.\n",
    "   The kappa formula also uses the expected propotion of chances of agreement.\n",
    "   \n",
    "   \n",
    "   kappa score=(Agree - chance agree)/(1-chance of agree)\n",
    "   \n",
    "   if agree=1 chance of agree , kappa score is 0,\n",
    "   if agree <chance of agree the kappa score is negative,\n",
    "   Denoting that the degree of agreement lower tahn chance of agreement\n",
    "   \n",
    "   \n",
    "   \n",
    "   To calculate chance of agree, we first look at the probability of rating a student as accept for eac professor \n",
    "   \n",
    "   For Accept prob  \n",
    "   professor a 6 of 25  ratings wete accepted, Then \"prob A(accept)=6/25=0.24\"\n",
    "                                                          \"prob B(Accept) =13/25 =0.25\"\n",
    "                                                          \n",
    "                                                          \n",
    "   For   Waiting   \"prob A(wait)=10/25=0.05\n",
    "                   \"prob B(wait)=3/25=0.48\n",
    "                   \n",
    "   For reject   \"prob A(reject)=9/25=0.018\n",
    "                \"prob B(reject)=9/25=0.018\n",
    "                \n",
    "    chance of agreement (accept)= probA(accept)*probB(accept)= 0.1248\n",
    "    like wise   chance of agree ment for (waiting) =probA(reject)*probB(reject)=0.125\n",
    "                                                    probA(waitint)*probB(waiting)=0.15\n",
    "                                              \n",
    "                                   Then total chance of agreement is =0.3024\n",
    "                                   \n",
    "                 kappa score=(Agree - chance agree)/(1-chance of agree)\n",
    "                            =0.48-0.3024/(1-0.3024)\n",
    "                            =0.25\n",
    "                                              \n",
    "      \n",
    "   \n",
    "         "
   ]
  },
  {
   "cell_type": "raw",
   "id": "594c8f4c",
   "metadata": {},
   "source": [
    "6. Describe the model ensemble method. In machine learning, what part does it play?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7da3022f",
   "metadata": {},
   "source": [
    "ensemble method is a ML technique that combines several base models in order to produce one optional predictive model.Types of ensemble method bagging or bootstrap, Aggregation , Boosting and Stacking.\n",
    "ensemble methods use multiple learning algorithms to obtain better predictive performance then could be obtained from any of the constituent learning algorithm.\n",
    "A ML ensemble consists of only a concrete finite set of a;lternatives models but typically allows for much more flexible structure to exist among  those alternatives."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f08070f9",
   "metadata": {},
   "source": [
    "7. What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to solve."
   ]
  },
  {
   "cell_type": "raw",
   "id": "57ef83f6",
   "metadata": {},
   "source": [
    "Descriptive modeling is a mathematical process that describes real world events and the relationship between factors responsible for them.\n",
    "The process is used by consumer driven organizations to help them target their marketing and advertising  efforts.\n",
    "   In descriptive modeling, customer groups are clusterd according to demographics , purchasing behaviour, expressedinterst and other descriptive factors .Statistics can identify where the cluster customer groups share similarities and where they differ\n",
    "   The main aspects of descriptive modeling are customer segmentation,value based segmentation,Behaviour based segmentation and need based segmentation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "29412296",
   "metadata": {},
   "source": [
    "8. Describe how to evaluate a linear regression model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d68872d",
   "metadata": {},
   "source": [
    "We can evalute a regression model using two metrics they are R-squared and Adjusted R-squared.\n",
    "The formula of R-squared metrics is given as \n",
    "     R-squared=1-(RSS/TSS)\n",
    "   where 'RSS' residual summation of square. \n",
    "         'TSS' Total summation squaare.\n",
    "         \n",
    "      RSS= summation(i=1 to n)(y- y hat)^2\n",
    "      TSS= summation(i=1 to n) (y- y bar)^2\n",
    "   \n",
    "   where 'y' is actual value.\n",
    "         'y hat' is predicted value.\n",
    "         'y bar' is mean of y.\n",
    "         \n",
    "    If RSS value increases the RSS/TSS will increase then R-squared value wil decrease.\n",
    "    If RSS value decreases, Then RSS/TSS value also decreases then R-squared value will increase.\n",
    "    \n",
    " It is not good metric when we increase or decrease number of features then its value will increse or decrase model accuracy,if that feature is important or unimporatant to build a model. To over come these problem we ADJUSTED-R-squared metric.\n",
    "      The formula of adjusted R-squared metric is given as \n",
    "      \n",
    "          ADJUSTED -R-squared  = (1-R-squ)(1-N)/(N-P-1)\n",
    "          \n",
    "            where 'P' is the number of features or columns.\n",
    "                  \"N' is the number of datapoints.\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "009bd2b4",
   "metadata": {},
   "source": [
    "9. Distinguish :\n",
    "\n",
    "1. Descriptive vs. predictive models\n",
    "\n",
    "2. Underfitting vs. overfitting the model\n",
    "\n",
    "3. Bootstrapping vs. cross-validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1587a511",
   "metadata": {},
   "source": [
    " Descriptive vs. predictive models:\n",
    "     a descrptive model describes a system or other entity and relationships to its environment. It is generally help specify and or understand what the system is what it does and it does.\n",
    "     Predictive modeling is commonly used statistical technique to predict future behaviour . Predictive modeling solution are aform of data mining technology that works by a nalyzing historical  and current data and generating a model to help predict future outcomes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cdb9b53",
   "metadata": {},
   "source": [
    " Underfitting vs. overfitting the model:\n",
    "         your model is overfitting you are training data  where you see that the model performs well on the training data but doesnot perform well on the evaluation data , this is because the model is memorizing the data it has seen and unable to generalize to unseen examples.\n",
    "         Your model is underfitting the training data , when the model perfoms poorly on the traing data .This is because of the model is unable to captoure the relation ship between the input and output features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dcb5a84",
   "metadata": {},
   "source": [
    " Bootstrapping vs. cross-validation :\n",
    "      Particularly useful for assesing the quality of ML model bootstrapping is a method of inferring results for a population on form results found on a collection of smaller random samples of the population using replacement during sampling process.\n",
    "        Cross validation some time called rotation estimation or out of sample testing. Is any of various similar model vqalidation techniques assigned not."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f7ba890",
   "metadata": {},
   "source": [
    "\n",
    "10. Make quick notes on:\n",
    "\n",
    "1. LOOCV.\n",
    "\n",
    "2. F-measurement\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb98eea1",
   "metadata": {},
   "source": [
    "LOOCV:\n",
    "    The leave one out cross validation or LOOCV , procedure is used to estimate the performance of machine learning algorithm, when they are used to make predictions on data not used to train the model , Leave one out cross validation is a configuration of k-fold cross validation , where k is set of number of examples in dataset.\n",
    "    LOOCV is extreme version of k-fold cross validation that has the maximum computational cost .It requires one model to be created and evaluated for each example in the training dataset .It is not appropriate for very large datasets.\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "575f2123",
   "metadata": {},
   "source": [
    "F-measurement: \n",
    "     It is used for calculationg performance of classification model.\n",
    "     The F-measurement is called as the harmonic mean of precision and recall ,giving each the same weighting.\n",
    "     It allows a model to be evaluated taking both the precision and recall into account using a single score, which is helpful when describing the perforance of the model.\n",
    "     When you are confusion ,which metric can use for a classification model in that cases you can use F-measurement.\n",
    "     \n",
    "     formula is given as \n",
    "     F=2PR/(P+R)\n",
    "       where 'P' is precision,\n",
    "             \"R' is recall."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb5dc5b",
   "metadata": {},
   "source": [
    "The width of the silhouette:\n",
    "       silhoette coefficient or silhouette swcore is a metric use to calculate the goodness of a clustering technique its values range from -1to 1.\n",
    "       1 means clusters are well apart from each other and clearly distinguished.\n",
    "       0 means clusters are in different or we can say that the distance between  cluster is not significant.\n",
    "       -1 means clusters are assigned in wrong way \n",
    "       \n",
    "     silhoette score formula is given as \n",
    "     \n",
    "        silhouette score=b-a/(max(a,b))\n",
    "      \n",
    "      where 'a' average intra cluster distance(average distance between each point within a cluster)(\n",
    "            'b' average inter cluster distance i.e the average distance between all clusters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cb86080",
   "metadata": {},
   "source": [
    " Receiver operating characteristic curve:\n",
    " \n",
    "    In  classifiction models , the threshold value is not a fixed one , it will changes with respect to  problem specification.In these cases how to select a best threshold value ?. \n",
    "         Finding a best threshold value will be done by using Reciever operartor chacteristics curve.\n",
    "       this is simply denoted as ROC. It is nothing but the relation ship between TPR(true positive rate) and FPR(False positive rate).\n",
    "          For a given dataset finding the TPR and FPR values and plotting a graph FPR as x-axis and TPR as y-axis  we can selcct best threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af7abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
