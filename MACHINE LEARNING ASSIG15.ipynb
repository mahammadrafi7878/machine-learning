{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ac6ce61e",
   "metadata": {},
   "source": [
    "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "daefc91a",
   "metadata": {},
   "source": [
    "Supervised machine learning is a process where you have input variable(x) and output variable(y) and you use an algorithm to learn the mapping function the input to the output y=f(x). The goal is approximate the mapping function so well that you have new input data(x) , That you can predict the ouput variables (y) This is called supervised learning.\n",
    "In supervised learning we have classification and regression problems .\n",
    "\n",
    "Unsupervised learning is a machine learning technique , Where you dont need to supervise the model. Instead you need to above the model to work on which discover information .It mainly deals with unlablled data .Unsupervised machine learning problems can be further divided into clustering association problems.\n",
    "\n",
    "The biggest difference between the supervised and unsupervised ML is , Supervised ML algorithms are trained on datasets that include laqbels added by ML engineerguide the algorithm to understand which features are important to the problm at hand it is very costly especially when dealing with large number of volumes of data.\n",
    "Unsupervised ML algorithms on the other hand , are trained on unlabeled data and must determine feature importance on their own based on inherent patterns in the data, the most basic disadvantage of any unsupervised ML it's application spectrum is limited..\n",
    "\n",
    "\n",
    "To counter this advantages , The concept of semisupervised learning was introduced . In this type of learning , The algorithm is trained upon a combination of labeled and unlabeled data. typically this combination will contain a very small amount of labeled data and very large amount of unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2da59910",
   "metadata": {},
   "source": [
    "2. Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bc5d4ed",
   "metadata": {},
   "source": [
    "Five examples of classification problems are 1.LOGISTIC REGRESSION, 2.DECISION TREE CLASSIFIER, 3.SUPPORT VECTOR CLASSIFIER ,4.NAIVE BAYES and 5RANDOM FOREST\n",
    "\n",
    "\n",
    "LOGISTIC REGRESSION:\n",
    "     Logistic regression it is basically a classification problem , as name suggests it is not a regression problem. In this algorithm the algorithms are correctly classified by a plane or hyperplane.\n",
    "     \n",
    "DECISION TREE CLASSIFIER:\n",
    "     It is a very poerful algorithm.This divides or seperates data based on entorp or gini impurity.It is tree base hierarchcal structure.It contains nodes , branches to divide the data set based on condition.\n",
    "     \n",
    "SUPPORT VECTOR CLASSIFIER:\n",
    "    support Vector Classifier , It becomes very popular in 1990's . It divides data using plane or hyperplane having maximize margin or distance between support vectors planes . \n",
    "    \n",
    "NAIVE BAYES CLASSIFIER:\n",
    "   It is a probabilistic based algorithm , it is very useful for text classification data ,It works based on bayes theorem and naive assumptions.\n",
    "   \n",
    "RANDOM FOREST CLASSIFIER:\n",
    "    Random Forest Classifier is one of ensemble techniques model. In this algorithm base algorithm isw decision trees , and ouput was taken majority vote rule.\n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "55d98a44",
   "metadata": {},
   "source": [
    "3. Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b2a9d3a",
   "metadata": {},
   "source": [
    "COLLECTING DATA:\n",
    "     machines initially learn form the data that you give them. It is the utmost importance to collect reliable data so that ML model can find the correct patterns.\n",
    "PREPARING THE DATA:\n",
    "      Putting to gether all the data you have and randomizing it, cleaning the data to remove unwanted data, missing values, row and columns ,duplicated values.Visualize data understand the relation ships between various variables and classes present.splitting the cleaned data into two sets  training set and testing set.\n",
    "      \n",
    "CHOOSING A MODEL: \n",
    "      It is important to choose a classification  model which is relevant to the task at hand.\n",
    "TRAINING A MODEL: \n",
    "     Training is the most important step. In training you pass the prepared data to your ML model to find pattern and make predictions.\n",
    "     \n",
    "EVALUATING MODEL: \n",
    "      After training your model, you have to check to see how it is performing.This is done by testing the performance of the model on previously unseen data. \n",
    "          some classifiaction model metrices are Confusion matrix, Precision,Recall,F1-score and AUC-ROC curve\n",
    "PARAMETER TUNING:\n",
    "Even you have created and evaluated model see if it's accuracy can be improved in any way , this is done by tuning parameters in you'r model.\n",
    "\n",
    "MAKING PREDICTIONS:\n",
    "    In the end you can use your model on unseen data to make predictions accurately."
   ]
  },
  {
   "cell_type": "raw",
   "id": "97075431",
   "metadata": {},
   "source": [
    "4. Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ca0f07d",
   "metadata": {},
   "source": [
    "With the help of support vector machines we can do both classification and regression algorithms.\n",
    "For classification we use SVC(Support vector classification).\n",
    "For Regression we use SVR(Support vector regression).\n",
    "we know the equation of line is y=mx+c.\n",
    "we can write line equation in quatradic function as ax+by+c=0. i.e  y=(-a/b)x - (c/b), here a/b and 'm' are called as coefficient or slope and 'c', 'c/b' is aclled as intercept points.\n",
    "We can write above quatradic equation as w1x1+w2x2+b=0, \n",
    "      i.e  w^tx+b=0,   where w^tx is [w1]  [x1 x2]\n",
    "                                     [w2]  \n",
    "                                     \n",
    "   If the line passing through origin then intercept will be Zero  i.e W^t=0.\n",
    "     Lets take a line  L and two points p1  above lineand p2 below line , Then the distance between line and points are  \n",
    "     \n",
    "     d=  w^t p1/ ||w||     is a positive value.   i.e ||w|| ||p1|| cosp.\n",
    "     d=  w^t p2/  ||w||   is a negative value      i.e  ||w|| ||p2|| cosp.\n",
    "     \n",
    "  There are many lines to divides data , In SVM we are trying to find a best fit line along with best create positive and negative marginal lines , such that distance between these two marginal line should be maximum.\n",
    "  These marginal lines are parllel to best fit line ,for postive point these line passing through neaest positive point and for negative point also same as positive line.\n",
    "     These points are called as support vectors. Using support vectors we can draw marginal lines.\n",
    "     \n",
    "     If the best fit line and marginal lines are clearly seperable data then it is called as 'HARD MARGIN\".\n",
    "     If the best fit line and marginal lines gives some error for seperating data then it is called as \"SOFT MARGIN\"\n",
    "     \n",
    "  we want distance between two marginal lines to be maximum then w^t(x1,x2)/||w||  equal to  2/||w||.\n",
    "  \n",
    "  cost function is minimize(w,b) ||w||/2\n",
    "  \n",
    "we can use some kernel tricks SVM for seperable nonlinear dataset. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "994cbe37",
   "metadata": {},
   "source": [
    "5. What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18ed18ca",
   "metadata": {},
   "source": [
    "The advantages of svm and support vectors:\n",
    "1.SVM works relatively well when there ia clear margin of seperation between classes.\n",
    "2.SVM is more effective in high dimensional space.\n",
    "3.It is effective in cases ,Where the number of dimensions is greater than the number of samples.\n",
    "4.SVM is relatively memory efficient.\n",
    "5.It work well fornon-linear data using svm -kernel's.\n",
    "\n",
    "DISADVANTAGES:\n",
    "1.SVM algorithm is not suitable for large datasets.\n",
    "2.SVM does not perform very well when the dataset has more noise i.e target classes are overlapping.\n",
    "3.In cases where number of features for each data point exceeds the number of training data samples,the svm will under perform.\n",
    "4.As the support vector classifier works by putting data points above and below there is no probabilistic explanation for the classification. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "86c7d298",
   "metadata": {},
   "source": [
    "6. Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b2af768",
   "metadata": {},
   "source": [
    "KNN means K-nearset neighbour it is a distance base approach. It is dont be considered as machine learning algorithm. Beacause it is not use mathematical equations or functions.\n",
    "KNN is a non parametric algorithm.\n",
    "It is called a slazy learner algorithm.It store whole dataset as a model. KNN algorithm is used for both classification and regression problems.\n",
    "KNN is a distance based approcah based on distance it makes distance.\n",
    "Lets take a binary classification , for giving new point then the point belongs to which class?\n",
    " step1:  The new point calculates distance with each and every point in dataset.\n",
    " step2: finding or selecting 'k' value .here 'k'value is hyperparameter.slecting 'k' values will be odd values , donot choose even value , if we even values then it is difficult to classify data point.\n",
    " step:\n",
    "    for classification problrm we choose majority vote rule and for regression problem selecting average of output vlues.\n",
    "    \n",
    "For selecting 'k' value we can use cross validation technique, oftenly we use 1. fold cross validation.\n",
    "For KNN algorithm we have high time complexity and need much more space.\n",
    "To over come these problem and reduce time complexity the varaition of KNN are available. For example : kD tree KNN and Ball tree KNN.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "260e5c94",
   "metadata": {},
   "source": [
    "7. Discuss the kNN algorithm's error rate and validation error."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7516244f",
   "metadata": {},
   "source": [
    "Using k-fold cross validation or cross validation we get best value of 'K' for eiether overfitting or under fitting.\n",
    "We know that accuracy is nothing but  correctly classified points diuvided by total number of points.\n",
    "     Accuracy =correctly classified points/total number of points.\n",
    "     \n",
    " From accuracy we can calculate ERROR Rate as   ERROR =1-Accuracy .\n",
    " To get maximum accuracy we need to redeuce error .\n",
    " \n",
    " Let's take a dataset and splitting into Dtrain, Dtest and Dcv (cross validation)\n",
    " \n",
    " Here using training dataset we find (NN) nearset neighbour, using Dcv we can find the value of error as \n",
    " \n",
    "     ERROR in Dcv =1-ACC in Dcv\n",
    "  To compute training error we use Dtrain for training dataset and for getting accuracy or error , we use Dtrain for training and Dcv for error accuarcy calculation then it is called as VALIDATION ERROR.\n",
    "  \n",
    "  In over fitting the validation error is high and training error is low . In uderfitting the validation error and training error will high."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2462293",
   "metadata": {},
   "source": [
    "8. For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "afd13386",
   "metadata": {},
   "source": [
    "Let's take a dataset and splitting into Dtrain, Dtest and Dcv (cross validation)\n",
    " \n",
    " Here using training dataset we find (NN) nearset neighbour, using Dcv we can find the value of error as \n",
    " \n",
    "     ERROR in Dcv =1-ACC in Dcv\n",
    "  To compute training error we use Dtrain for training dataset and for getting accuracy or error , we use Dtrain for training and Dcv for error accuarcy calculation then it is called as VALIDATION ERROR.\n",
    "  \n",
    "  In pverfitting the validation error is high and training error is low . In uderfitting the validation error and training error will high."
   ]
  },
  {
   "cell_type": "raw",
   "id": "184af66c",
   "metadata": {},
   "source": [
    "9. Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e35d9b8",
   "metadata": {},
   "source": [
    "KNN is called as lazy learner algorithm. let's take  a binary classification probelm and giving a new data point, the point blonge to whidh class? \n",
    "\n",
    "  lets take a  dataset  \n",
    "  x1  2   1   1   1   1   1   4\n",
    "  x2  4   8   9   8   1   10  10\n",
    "  y   1   2   1   2   1    1   2\n",
    "  \n",
    " KNN is a distance based approach , we calculate distance using euclidean distance \n",
    "     euclidean distance frormula is given as  (x1,y1),(x2,y2)\n",
    "           Then euclidean distance D= sqrt((x2-x1)^2 + (y2-y1)^2)\n",
    "      for above datset giving new data point x1=2, x2=1  then what is its class,\n",
    "    Then it will compare its distance with each and every point in dataset\n",
    "    \n",
    "    distance between given point x1=2,x2=1  with first record x1=2,x2=4   =3\n",
    "     distance between given point x1=2,x2=1  with second record x1=1,x2=8   =7.07\n",
    "      distance between given point x1=2,x2=1  with third record x1=1,x2=9   =8.06\n",
    "       distance between given point x1=2,x2=1  with fourth record x1=1,x2=8   =7.07\n",
    "      distance between given point x1=2,x2=1  with fifth record x1=1,x2=10   =9.05\n",
    "       distance between given point x1=2,x2=1  with sixth record x1=1,x2=1   =1\n",
    "        distance between given point x1=2,x2=1  with seveth record x1=2,x2=10   =9.05\n",
    "        \n",
    "  Then finding minimum three distance for this point , IT is a 'k' value . In KNN k is hyperparameter value we choose k value based on cross validation for this example taking k=3,\n",
    "    Then x1=1,x2=2 =1      y -value 1\n",
    "         x1=2,x2=4 =3      y value  1\n",
    "         x1=1,x2=8 =7.07   y value  2\n",
    "         \n",
    "     Then taking majority vote rule of y values , we have two 1 and one 2 , then the given new point belongs to class 1.1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "438261f2",
   "metadata": {},
   "source": [
    "10.What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcaf0986",
   "metadata": {},
   "source": [
    "A decision Tree is a non parametric supervised learning algorithm , which is utilised for both classification and regression tasks.It has a hierarchical , tree structure which consists of root node , branche, internal nodes and leaf nodes.\n",
    "A decision tree is a decision support tool that use a tree like model for decisions and their possible consequences, including chance event outcomes , resourse costs and utility . It is one way to display an algorithm that only contains conditional control statements.\n",
    "In decision tree we have root lode , leaf node and intermediate nodes.\n",
    "\n",
    "   Root nodes are Which does not have any incoming branches , the outgoing branches forom the root node then feed into the internal node also known as decision nodes.\n",
    "   Leaf nodes or Terminal nodes represents all possible cases with in a dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31dbbdf",
   "metadata": {},
   "source": [
    "11. Describe the different ways to scan a decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ef92d83",
   "metadata": {},
   "source": [
    "There are two different ways to scan a decision tree they are \n",
    "1.GINI IMPURITY\n",
    "2.ENTROPY \n",
    "Using these two scaning techniques in decision tree we can divide node and build a decsion tree.\n",
    "\n",
    "GINI IMPURITY:\n",
    "   The formula of  gini impurity is given as  \n",
    "       gini= 1 - sum(i = 1 to c) (pi)^2 \n",
    "          here 'c' is number of classes in a label.\n",
    "          \"pi\" is the probability of distinct points in feature with repeat to c.\n",
    "                   \n",
    "       After calculating gini impurity then we caluculate infomation gain as \n",
    "       \n",
    "       Information gain = sum(i=1 to g(c))  (G(c'))(number of instances for a class /total number of instances)\n",
    "       \n",
    "           here G(c')) is gini  value of each class in feature , \n",
    "           first we calculate the gini impurity for each class feature , then we sum up gini impurity for feature , The feature which has low gini impurity take as root node. Low gini impurity means high purity in feature .\n",
    "           like these way calculate gini for neach feature and slect a feature a with low gini impurity , repeat these steps untill we get a reasonble tree.\n",
    "           \n",
    " ENTROPY:\n",
    "      Entropy is nothing but randomness or degree of freedom i.e having high randomness means high entropy.\n",
    "      Highest flexibility of a movement is called as entropy.\n",
    "      Entropy formula is given as  \n",
    "           ENTROPY=summation(i=1 to c) (-pi) log(pi)\n",
    "           here 'c' is number of classes in a label.\n",
    "          \"pi\" is the probability of distinct points in feature with repeat to c.\n",
    "           \n",
    "       Information gain is calculated based on Entropy .Information gain is nothing but entropy befor and entropy after.\n",
    "          IG=Entropy (before) - Entropy(after)\n",
    "          \n",
    "       First we calculate entropy of dependent variable or target variable. after this caalculate entropy for each column and then calculate Information gain, A feature with high Information gain is selected for node.\n",
    "       "
   ]
  },
  {
   "cell_type": "raw",
   "id": "97fb7931",
   "metadata": {},
   "source": [
    "\n",
    "12. Describe in depth the decision tree "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4f70ba1",
   "metadata": {},
   "source": [
    "It is a more powerful than the linear regression and logistic regression.\n",
    "It is used for both classification and regression problems.\n",
    "A decision tree with classification problems called as decision tree classifier.\n",
    "A decision tree with regression problems called as decision tree regressor.\n",
    "Decision tree divides total dataset untill till last layer , which make binary decision tree tree.\n",
    "In decisionnn tree we have root node, leaf nodes, intermediate nodes and branches to divide the dataset.\n",
    "A root node is one which has no input layer and leaf node is one which doesnot have any ouput or no dataset to divides.\n",
    "The nodes whch are used to divide branches are called as the intermediate nodes.\n",
    "We divide dataset based on two approches.\n",
    "\n",
    "1.GINI IMPURITY\n",
    "2.ENTROPY \n",
    "Using these two scaning techniques in decision tree we can divide node and build a decsion tree.\n",
    "\n",
    "GINI IMPURITY:\n",
    "   The formula of  gini impurity is given as  \n",
    "       gini= 1 - sum(i = 1 to c) (pi)^2 \n",
    "          here 'c' is number of classes in a label.\n",
    "          \"pi\" is the probability of distinct points in feature with repeat to c.\n",
    "                   \n",
    "       After calculating gini impurity then we caluculate infomation gain as \n",
    "       \n",
    "       Information gain = sum(i=1 to g(c))  (G(c'))(number of instances for a class /total number of instances)\n",
    "       \n",
    "           here G(c')) is gini  value of each class in feature , \n",
    "           first we calculate the gini impurity for each class feature , then we sum up gini impurity for feature , The feature which has low gini impurity take as root node. Low gini impurity means high purity in feature .\n",
    "           like these way calculate gini for neach feature and slect a feature a with low gini impurity , repeat these steps untill we get a reasonble tree.\n",
    "           \n",
    " ENTROPY:\n",
    "      Entropy is nothing but randomness or degree of freedom i.e having high randomness means high entropy.\n",
    "      Highest flexibility of a movement is called as entropy.\n",
    "      Entropy formula is given as  \n",
    "           ENTROPY=summation(i=1 to c) (-pi) log(pi)\n",
    "           here 'c' is number of classes in a label.\n",
    "          \"pi\" is the probability of distinct points in feature with repeat to c.\n",
    "           \n",
    "       Information gain is calculated based on Entropy .Information gain is nothing but entropy befor and entropy after.\n",
    "          IG=Entropy (before) - Entropy(after)\n",
    "          \n",
    "       First we calculate entropy of dependent variable or target variable. after this caalculate entropy for each column and then calculate Information gain, A feature with high Information gain is selected for node.\n",
    "       \n",
    "       \n",
    " In decsion tree may have problem like overfitting underfitting .To over come these problems we use Post pruning and prepruning techniques.\n",
    " Diffeternt types of DT algorithms are CART,ID and c4.5.\n",
    " Decision tree regressor are also works well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec82071c",
   "metadata": {},
   "source": [
    "13. In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03aed81b",
   "metadata": {},
   "source": [
    "Before learning model from given data and learning algorithm , There are assumptions a learner makes about the algorithm. These assumptions are called as inductive bias.\n",
    "It is the property of the algorithm for e.g in the case of decision tree the depth of decision tree is the inductive bias .\n",
    "To stop over fitting of decision tree , we can do post pruning like giving maximum depth of tree as parameter."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8a4c4f1",
   "metadata": {},
   "source": [
    "14.Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2eff187",
   "metadata": {},
   "source": [
    "ccADVANTAGES:cc\n",
    "1.Theses rae simple to understand and interpret.\n",
    "2.Have value even with littile hard data.\n",
    "3.Help determine worest, best and expected values for different scenarios.\n",
    "4.Use a white box model.If a given result is provided by a model.\n",
    "5.Can be combined with other deision techniques.\n",
    "6.The action of more than one decision can considered.\n",
    "\n",
    "\n",
    "\n",
    "DISADVANTAGES:\n",
    "1.They are unstable , meaning that a small change in the data can lead to a large change in structure of optimal decision tree.\n",
    "2.They are often relatively inaccurate may other predictions perform better with similar data.\n",
    "3.Calculations can get very complex , particularly if many values are uncertain and if many out comes are linked\n",
    "4.For data including categorical variables with different number of levels ,information gain in decision tree is biased in favour of those attributes with more levels."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d7b5619",
   "metadata": {},
   "source": [
    "15. Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d47229db",
   "metadata": {},
   "source": [
    "They are commonly used by data analysts to carry out predictive analysis, They are also popular for machine learning and AI, where they are used as training algorithms for supervised learning.\n",
    "A technology business evaluating expansion opportunities based on analysis of past sales data.\n",
    "A toy company deciding where to target its limited advertising budget, based on what demographics data suggests customers are likely to buy.\n",
    "Banks and ,artage provides using historical data to predict how likely it is that a barrower will default on their payments.\n",
    "Automated telephone systems guiding you to out come you need  for example:for option A press1 , For option 2 Press 2  ...etc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12b77be8",
   "metadata": {},
   "source": [
    "16. Describe in depth the random forest model. What distinguishes a random forest?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49369907",
   "metadata": {},
   "source": [
    "Random forest model is one of the ensemble technique.It is used base algorithm as decision trees.\n",
    "Random forest is nothing but combination of multiple decision trees.It is a good idea to take ouput of one decision tree taking majority vote of the multiple decision tree.it correctly classify the points and improve accuracy also.\n",
    "In this technique we use bagging approcah , here we are create multiple data set with replacement .\n",
    "   i.e we nave a dataset with nxm , then we selct a dataset with n1<n and m1<m datpoints with replacement  like these datasets 'k' .\n",
    " for k number of datsets craeting k number of decision tress and taking ouput as a majority vote of k models.\n",
    " This is an efficient way , using these tecchnique we can reduce high variance models with out affecting bias.\n",
    "In Random forest we apply row sampling and columnor feature  sampling also.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
