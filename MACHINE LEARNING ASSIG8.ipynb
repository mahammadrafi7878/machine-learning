{
 "cells": [
  {
   "cell_type": "raw",
   "id": "73303da2",
   "metadata": {},
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e311f1c",
   "metadata": {},
   "source": [
    "In machine learning and pattern recognition , a feature is an individual measurable property or characteristics of phenomenon. \n",
    "Feature is a column or variable or attribute in a dataset . It is a meaurable property or carry some information about the dataset. \n",
    "Feature are be different data types Quantitative or Quaalitative. \n",
    "Choosing informative ,disciminiting and independent feature is crucial elements of effective algorithms in pattern recognition,Classification and regression features are usually numeric but structured features such as string graphs are synthetic pattern recognition.\n",
    "The concept of feature is related to that exploratory variable used in statistical techniques such as linear regression.\n",
    "In speech recognition , features for recognition phenemenon can include noise ratio, length of sounds, iterative power,filters matches and many others."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0eec58f",
   "metadata": {},
   "source": [
    "2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "629c6e45",
   "metadata": {},
   "source": [
    "Feature construction (also known as constructive induction or attribute discover) enriches data by adding derived features.\n",
    "Feature construction is a process which build intermediate features from the original descriptors in a dataset.\n",
    "It is the application of a set of constructive operations to set a existing features resulting in construction of new features.\n",
    "higher level features can be obtained from already available features and added to feature vector for example , for the study of diseases the feature 'AGE' is useful and is defined as age =year of death - year of birth . This process is reffered to as feature construction. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4a620f0",
   "metadata": {},
   "source": [
    "3. Describe how nominal variables are encoded."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f1d5c56",
   "metadata": {},
   "source": [
    "When we are working with on some of datsets ,we found that some of the features are categorical .We all know that machines are cannot understand categorical data.Models work with numerical values.\n",
    "It is hence say to convert the categorical values into numerical values.This process of converting categorical data into numerical data is called ENCODING.\n",
    "\n",
    "NOMINAL ENCODING: \n",
    "    When we have a feature where variables are just names and there is no order or rank to this variables features example: marital status, city of person lives.\n",
    "    For nominal data we can use one hot encoding . for example variables where no ordinal relation ship exists , The integer encoding may not be enough or mislaeding the worest order.\n",
    "    Each bit represents a bit represents a possible category. If the variable cannot belong to multiple categories at once, Then only one bit in the group can be 'ON' this is called one hot encoding. \n",
    "    \n",
    " Example: \n",
    " In the color variable there rae three categories and therfore three binary variables are needed. a '1' value is placed in the binary variable for the color and '0' values for other colors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0455762",
   "metadata": {},
   "source": [
    "4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ba74b05",
   "metadata": {},
   "source": [
    "We can ose some of feature engineering techiniques to convert numeric features into categorical features.\n",
    "One of them is using Indicator Variable.\n",
    "Let take a numeric feature height ,then putting threshold for height feature as if height >150  is one and height <150 0 then it will converted into categorical feature.\n",
    "We can give multilple thrsholds also . This process is called as 'BINNING\".\n",
    "It is also known as discretization , It is the process of transforming continuoyus variable into categorical variable. By creating a set of intervls , which are contiguous , That span over the range of the variable values.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8af9793",
   "metadata": {},
   "source": [
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6edd3fe3",
   "metadata": {},
   "source": [
    "When we have a 100+ features in a dataset that is ridiculous cannot to process normally which where feature selection methods come in handy,\n",
    "They allow you to reduce the number of features included in a model without sacrifising the predictive power.\n",
    "Features that are redundant or irrelevent can actually negatively impact your model performance , So it is necessary to remove them.\n",
    "There are three types of feature selection .\n",
    "\n",
    "1.WRAPPER METHODS(Forword, back word and stepwise).\n",
    "2.FILTER METHODS(ANOVA, Pearson's correlation coefficient,Variance threshold)\n",
    "3.ENSEMBLE METHODS(Lasso,Ridge and Decision tree)\n",
    "\n",
    "\n",
    "Feature selection WRAPPER approach:\n",
    "      Wrapping methodes compute models with a certain subset of features and evaluate the importance of each feature.Then they iterative and try a different subset of features untill the optial subset reached.\n",
    "      The most notable WRAPPER of feature selection are  1.FORWORD  SELECTION, 2.BACKWORD SELECTION AND 3.STEPWISE SELECTION \n",
    "      \n",
    "ADVANTAGES:\n",
    "1.The main benfit of feature selection is that it reduces overfitting by removing extraneous data\n",
    "2.Removing irrelevent information i.e improves the accuracy of the model prediction.\n",
    "3.It also reduces computational time involved to get model.\n",
    "\n",
    "DRAWBACKS:\n",
    "Large computational time for data with many features and , it tends to overfit the model when there is not large amount of data points ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ee7656a",
   "metadata": {},
   "source": [
    "6. When is a feature considered irrelevant? What can be said to quantify it?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b80459e",
   "metadata": {},
   "source": [
    "A feature Xi is said to be relevant to a concept C if Xi appears in every boolean formula that represents C, If it is not then it will be said as irrelevant.\n",
    "like wise Feature can be regarded as irrelevant if it is conditionally independent of the class labels or it doesnot influence the class labels.\n",
    "\n",
    "\n",
    "The core detecting irrelevant feature is to find whether the ouput is affected by a given feature .\n",
    "There are several mathematical calculations can be used based on type of output features.\n",
    "   Categorical feature --- categoriacal feature  ( use chi square)\n",
    "   Categorical feature --- continuous response (use ANOVA)\n",
    "   Continuous feature ---- continuous respnse (chi square or ANNOVA)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b08c4cbc",
   "metadata": {},
   "source": [
    "7. When is a function considered redundant? What criteria are used to identify features that could be redundant?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de3cf706",
   "metadata": {},
   "source": [
    "A redundant function is one which has potentially been superseded by another function and should not be used any more.\n",
    "This is apparent, The number of features is positively related to training time, The more features you have, The slower calculations.\n",
    "One popular way to detect multicollinearity in data is called \"Eigen system analysis, which uses the concept of condition number . The defination of condition number is k= lam max/lam min\n",
    "\n",
    "    k= largest eiegen value/samllest eigen value.\n",
    "    \n",
    "    \n",
    " Or given a matrix find the correlation of matrix X is \n",
    "         cov(Xi,Xj)/sig(Xi)*sig(Xj)\n",
    "         \n",
    "    If the correlation matrix has a large  condition number , it indicates serious collinearity.\n",
    "    \n",
    " >Redundant features slow down the training process\n",
    " >Reduce estimation abilities\n",
    " >Model is hard to interpret"
   ]
  },
  {
   "cell_type": "raw",
   "id": "068b34db",
   "metadata": {},
   "source": [
    "8. What are the various distance measurements used to determine feature similarity?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a2d52b4",
   "metadata": {},
   "source": [
    "Nine distance measure are 1.Euclidean distance, 2.Cosine similarity, 3.Hamming distance, 4.Manhattan distance,\n",
    "5.Cheyshev distance,6.Minikowski distance, 7.Jaccard index,8.Haversine and  9.Sorension dice index."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce16a51f",
   "metadata": {},
   "source": [
    "9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bda3465",
   "metadata": {},
   "source": [
    "EUCLIDEAN:\n",
    "1.It is most common distance measure , that best can be explained as the length of a segment connecting two points.\n",
    "2.Euclidean distance works greater when you have low dimensional data and magnitude of the vectors is important to be measured.\n",
    "3.Methods like KNN and HDBSCAN show great resultsout of the box if euclidaen distance is used on low dimensional data.\n",
    "4.Euclidean distance is not scale in-varient, Which means that distance computed might be skewed depending on the features.\n",
    "5.We need to normailize the data before using distance measure.\n",
    "\n",
    "MANHATTAN: \n",
    "     1.It is often called as taxicab distance or city block distance , caluculates the distance between real valued vectors.\n",
    "     2.Manhattan distance refers to the distance between two vectors if they could only more right areas.\n",
    "     3.When your dataset has discrete and binary attributes , manhatton to work quite well , Since it takes into account the paths taht realisticly could be taken within values of the attributes.\n",
    "     4.Manhatton distance seems to work okay for highdimensional data.\n",
    "     5.It is more likely to give a higher distance value , Than euclidean distance since it does not shortest path possible."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4660d7f5",
   "metadata": {},
   "source": [
    "10. Distinguish between feature transformation and feature selection."
   ]
  },
  {
   "cell_type": "raw",
   "id": "27e75396",
   "metadata": {},
   "source": [
    "Feature transformation , transforms data to improve the accuracy of the algorithm.\n",
    "Feature selection will removing unnecessary features .\n",
    "Feature Transformation  are Normalizing and changing distributions , These algorithms are roboust to unusual distributions.\n",
    "Feature selection , the more the data , The higher the computational complexity , some algoriyhms may take noise a signal and overfit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "de71fd2f",
   "metadata": {},
   "source": [
    "11. Make brief notes on any two of the following:\n",
    "\n",
    "          1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f8ca37c",
   "metadata": {},
   "source": [
    " The width of the silhouette:\n",
    "  Let i be  a focal object belonging to cluster A. Denote by C is a cluster not containing i.\n",
    "  a(i) is defined as the avrage dissimilarity between i and all otherobjects in A.\n",
    "  While c(i,C) is the average disimilarity between i and all objects in c.\n",
    "     \n",
    " b(i)=min (c =! A) c(C,i)\n",
    "       \n",
    "     silhoette width formula is given as \n",
    "     silhouette width s(i) =b(i)-a(i)/(max(a(i),b(i)))\n",
    "      \n",
    "     \n",
    " s(i) ranges form -1 and 1.\n",
    " 1 indicate that object i is much closer to the other objects in the closest other cluster.\n",
    " 0 the classification of the focal object is doubtful.\n",
    " -1 indicates missclassification of points.\n",
    "     \n",
    "      \n",
    "      \n",
    "      Receiver operating characteristic curve:\n",
    "\n",
    "\n",
    " Receiver operating characteristic curve:\n",
    " \n",
    "    In  classifiction models , the threshold value is not a fixed one , it will changes with respect to  problem specification.In these cases how to select a best threshold value ?. \n",
    "         Finding a best threshold value will be done by using Reciever operartor chacteristics curve.\n",
    "       this is simply denoted as ROC. It is nothing but the relation ship between TPR(true positive rate) and FPR(False positive rate).\n",
    "          For a given dataset finding the TPR and FPR values and plotting a graph FPR as x-axis and TPR as y-axis  we can selcct best threshold value.\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
