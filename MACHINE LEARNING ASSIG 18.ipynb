{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a4b5c26f",
   "metadata": {},
   "source": [
    "1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "raw",
   "id": "caed06d8",
   "metadata": {},
   "source": [
    "1.supervised learning algorithms are trained using labelled data . Unsupervised learning algorithm are trained using unlabeled data.\n",
    "2.Supervised learning model takes direct feed back to check if it is predicting correct output or not.Unsupervised learning doesnot take any feed back.\n",
    "3.Supervised learning predicts the output.Unsupervised laerning predicts the hidden pattern of data.\n",
    "4In supervised laerning input data is provided to the model algorithm with the output, In unsupervised learning only input data is provided.\n",
    "5.The goal of supervised laerning is to train model so that it can predict the output when it is given new data. The goal of unsupervised learning is find the hidden pattern and useful insights from unknown data.\n",
    "6.Supervised Ml can be classified into two categories classification and regression. Unsupervided ML classified into clustering and assocuation problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bebc5f32",
   "metadata": {},
   "source": [
    "2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eacd6461",
   "metadata": {},
   "source": [
    "1.Product Segmentation.\n",
    "2.Customer Segmentation.\n",
    "3.Similarity Segmentation.\n",
    "4.Recommendation Systems.\n",
    "5.Label encoding and unlabeled dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b080a6",
   "metadata": {},
   "source": [
    "3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1d09dd9",
   "metadata": {},
   "source": [
    "The three main clustering algorithms are \n",
    "1.k-mean clustering.\n",
    "2.Hirarchical clustering.\n",
    "3.DBSCAN(density based spatial clustering of application with noise )clustering.\n",
    "\n",
    "\n",
    "K-MEAN CLUTERING: \n",
    "   The k-mean clustering performs two tasks . Determines the best value for k-center points or centroids by an iterative process.\n",
    "   Assign each data point to its closet k-center .Those data points which are rear to the particular k-center create a cluster.\n",
    "   \n",
    "HIERARCHICAL CLUSTERING: \n",
    "    Hierarchical clustering methods summerizes the data hierarchy i.e they construct a number of local data points that are eventually nested.'The clustering outcome depends on the selected linkage strategies(single,complete and averagesentriod) and the same similarity measure being considered.\n",
    " \n",
    " DBSCAN CLUSTERING:\n",
    "     DBSCAN stand for density based spatial clustering of application with noise) it is able to find arbitary shaped clusters and clusters with noise. The main idea befind DBSCAN is that a point belongs to a cluster of it is close to many points from the cluster."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ab20d1",
   "metadata": {},
   "source": [
    "4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c77506bf",
   "metadata": {},
   "source": [
    "K-means clustering algorithm find similarity between two points using  distance base  approach algorithm. It firstly find the optimal value of 'k'.\n",
    "If a datset have 'n' number of datapoints then we can create 'n' number of clusters or groups.\n",
    "It approches WCSS(with in cluster summation of square ) method.\n",
    "In k-means algorithm 'k' is value represents number of clusters we want or number of centroids.\n",
    "To find how many cluster can form or number of groups can create we use ELBOW method  or Inertia method .\n",
    "for example if we take 'k' value as  2, i.e means two data points randomly from the dataset.\n",
    "For the first point  centroid is first point and for second point centroid is second point.\n",
    "In nexxt step these two data points search for their nearest points ,it add a datapoint which nears to them.\n",
    "After adding datapoint then the centroid of the cluster will be change .Thses e Three steps are repeated until all data points are grouped.\n",
    "\n",
    "To check if the points are clusterd correctly we use INTRA and INTER cluster distance .\n",
    "Intra cluster the average distace between the points between two cluster.Intra cluster meand the average distance between data points with in a cluster.\n",
    "Intra cluster distance will be maximum and Inter cluster distance will maximum ,Then the points are correctly grouped.\n",
    "K-means cluster evaluation metrics generally used are  DUNN index and Inertia method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f125f02",
   "metadata": {},
   "source": [
    "\n",
    "5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3774d457",
   "metadata": {},
   "source": [
    "K-MEANS:\n",
    "K-means minimizes within cluster variance , which equals squaared euclidean distance.In general arithematic mean does this , it does not optimize distances, but squared deviations from the mean.\n",
    "K-MEDIANS:\n",
    "K-medians minimizes absolute deviation , which equals manhttan distances , in general the per axis median should do this , It a good estimator for the mean of you want to minimize the sum of absolute deviations, instead of squared erros\n",
    "1.If your distance is squared euclidean distances use k-means , If you are distances is taxicab metric use k-meadians\n",
    "2.k-means attepmt to minimize the total squared error , where as k-meadians minimizes the sum of disimilarity between points labeled to be in a cluster and a point designated as center of that cluster."
   ]
  },
  {
   "cell_type": "raw",
   "id": "45fb0589",
   "metadata": {},
   "source": [
    "6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "48dbe98e",
   "metadata": {},
   "source": [
    "dendogram is nothing but a representation of hierarchical clustering.In this cluster points which are similar to each other they form as cluster.\n",
    "For each and every possibility cobines together and forms new cluster, untill a single cluster is obtained.\n",
    "Based on dendogram we can define whether it is agglomarative clustering or divise clustering.\n",
    "In dendogram we take eac point as a cluster, Then the pointsb which are similar to each other and form a new cluster for every step.\n",
    "Thisprocess is repeated untill a single cluster obtained , This process is called as dendogram .\n",
    "In this process we are going fron 'n' of cluster to a single cluster then it is called as AGGLOMERATIVE(bottom to top approach) Clustering.\n",
    "In this process if we consider whole dataset as a cluster then dividing this cluster into 'n' number of clustrs is called as i.e top to bottom approach is called as DIVISIVE  clustering."
   ]
  },
  {
   "cell_type": "raw",
   "id": "013f4e65",
   "metadata": {},
   "source": [
    "7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34b5e4d8",
   "metadata": {},
   "source": [
    "Intra cluster variance also known as the squared error or sum of squares within(SSW) or sum of squares error(SSE) is used to quantify internal cohension.\n",
    "It is defined as the sum of squared distances between the average point (centroid) and each point of the cluster , The smaller the value better the clustering.\n",
    "\n",
    "       Intra cluster variance=summation(summation1(Distance(x,centroid)^2 \n",
    "        where 'cluster' is centroid of given cluster.\n",
    "              'summstion'  number of cluster.\n",
    "              'summation1'  number of data points given cluster.\n",
    "              'x' is a data point in given cluster.\n",
    "              \n",
    "      The objective of k-means is to put data points with similar characteristis in the same cluster and seperate data points with different characteristics into different cluter, To quantify both internal cohension and external seperation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aeed4c36",
   "metadata": {},
   "source": [
    "8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "raw",
   "id": "41d511b4",
   "metadata": {},
   "source": [
    "K-means algorithm find the similarity between to points based on its disatance .The main procedure and steps in  k-means algorithm are ,\n",
    "STEP1:find out centriod Initially taking 'n' number of datapoints randomly and finding their centroids.\n",
    "    Here we are selecting 'k' value randomly , for finding the best value of k we can use ELBOW method.\n",
    "STEP 2:\n",
    "   Calculating euclidean distance between each every point in the datset.which gives less distance then it will belong that centroid.\n",
    "   \n",
    "STEP 3:  After adding a point to cluster we need to update clusters centroid.\n",
    "STEP 4: Iterate untill all the datapoints."
   ]
  },
  {
   "cell_type": "raw",
   "id": "24ec9820",
   "metadata": {},
   "source": [
    "9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "raw",
   "id": "27694c6f",
   "metadata": {},
   "source": [
    "In the single link or single linkage hierarchical clustering we merge in each step the two cluster whose two closest members have the smallest distance or the two clusters with the smallest minimum pairwise diatance .Single linkage is the shortest distance between a pair of observations in two clusters .It can sometimes produce clusters , ehere observations in different clusters are close togethr than to observations within their own clusters .These cluster can appear spread out.\n",
    "In complete linkage is where distance is measured between the farthest pair of observations in two clusters. This method usually produces higher cluster than single linkage but these tight cluster canend up very close together."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d71c6e60",
   "metadata": {},
   "source": [
    "10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "raw",
   "id": "35ddb9dc",
   "metadata": {},
   "source": [
    "Apriori algorithm assumes that any subset of frequent item must be frequents ,Its the algorithm behind market basket analyss.\n",
    "Let's take a dataset,\n",
    "   say a transaction (grapes,apple,magoes) also conatins (grapes,mango} so,according the principle of apriori if{grpes,apple,mango} is frequent then {grapes,mangoes} must also be frequent.\n",
    "   Here is a data set consition of six transactions each transaction is a combination od 1 and 0 , where 0 represents sent absense transaction and 1 represents the presense of transaction.\n",
    "   \n",
    "   \n",
    "  transaction id   grapes   apple    mango    orange  \n",
    "    1               1         1       1         1\n",
    "    2               1         0       1         1\n",
    "    3                0        0       1         1  \n",
    "    4                1        1       1         1\n",
    "    5                0        1       0         0\n",
    "    6                1        1       0         1\n",
    "    \n",
    "  In orfer to find at intersecting rules of multiple possible rules from the small business scenrio, we will using the following metrics \n",
    "  \n",
    "  \n",
    "SUPPORT:  \n",
    "     It is default popularity of an item in mathematical terms , the support of item A is nothing but the ratio of transactions involving A to the total number of tranactions.\n",
    "     \n",
    "     SUPPORT(Grapes)=Transactions involving grapes/ total number of transaction\n",
    "                    =0.666\n",
    "                    \n",
    "CONFIDENCE: \n",
    "      likelihood that customer who bought both Aand B divides the number of transactions invoving both A and B by the transaction involves B.\n",
    "         Confidence(A=>B)  =Transaction involving A and B/ Transaction involves only A\n",
    "         \n",
    "         confidence({grapes,apples=>mango)=  support grapes,apple,mango/support grapes,apples\n",
    "                                          =0.667\n",
    "                                          \n",
    "  LIFT: \n",
    "   Increases in the sale of A when you sell B , Lift(A=>B)(= confidence(A,B)/support (B)\n",
    "   \n",
    "      if Lift(A=>B)=1 means that there is positive correlation with in dataset.\n",
    "         Lift(A=>B)>1   means that there is a positive correlation with in the itemset i.e products in the item set A and B are more likely to be Brought to gether\n",
    "         Lift(A=>B)<1  , means that there is a negative correalton within the itemset i.e products in itemset A and B are unlikely to be bought to gether."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
