{
 "cells": [
  {
   "cell_type": "raw",
   "id": "448dcacd",
   "metadata": {},
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "162b2096",
   "metadata": {},
   "source": [
    "reasons for reducing dimensionality of a dataset:\n",
    "1.To speed up a subsequent training algorithm(in some cases it may even remove noise and redundant featurs )\n",
    "2.Making the training algorith perform better.\n",
    "3.To visualize the data and gain insights on the most important features simply to save space.\n",
    "\n",
    "DISADVANTAGES:\n",
    "1.Some information is lost ,possibly degrading the performance of subsequent training algorithm.\n",
    "2.It can be computationally intensive , it add some complexity to your machine learning pi[peline.\n",
    "3.Transformed feature are often hand to interpret."
   ]
  },
  {
   "cell_type": "raw",
   "id": "71b67100",
   "metadata": {},
   "source": [
    "2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3aea1ca",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the fact that many problems that do not exist in low dimensional space araise in high dimensional space.\n",
    "In machine learning one common manifestation is the fact that randomly sampled high dimensional vectors are generally very sparse , Increasing the risk of overfitting aand making it very difficult to identify patterns in the data without having plenty of training data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eef92d6",
   "metadata": {},
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "383b6a1a",
   "metadata": {},
   "source": [
    "Once a  dataset's dimensionality has been reduced using one of the algorithm , that we discussed , It is almost impossible to perfectly reverse the operation, because aome information gets lost during dimensionality reduction.\n",
    "More ever while some algorithm have a simple reverse transform ation procedures that can be construct a dataset relatievly similar to the original algorithm.\n",
    "Using PCA algorithm we can do , but using t-SNE we can not do."
   ]
  },
  {
   "cell_type": "raw",
   "id": "77d456e7",
   "metadata": {},
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ff50872",
   "metadata": {},
   "source": [
    "PCA can be used  significantely to reduce the dimensionality of most datasets even if they are heaqvily non linear , Because it can atleast get rid of useless dimensions.\n",
    "However if there are no useless for example the swist roll then reducing dimensionality with PCA will lose too much information you want to unroll the swiss roll not sqaush it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "39a6cf03",
   "metadata": {},
   "source": [
    "5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ee669ec",
   "metadata": {},
   "source": [
    "It is depends on the datset, let's look at two extrme examples , first suppose the data set is composed of points that are almost perfectly aligned.\n",
    "In this case PCA can reduce dataset down to just one dimension while still preserving 95% of the variance.\n",
    "Now imagine that the dataset is composed of perfectly random points , scrolled around 1000 dimensions In this case all 1000 dimensions are required to preserve 95% of the variancde.\n",
    "So the another is it depends on the dataset and it could be any number between 1 to 1000 plotting the \n",
    "explained variance as a function of the number of dimensions is one way to get rough idea of dataset's intrinsic dimensionalities."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e82e3cd0",
   "metadata": {},
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84dc673e",
   "metadata": {},
   "source": [
    "regular PCA is the default , but it works only if the dataset fits in memory.\n",
    "Incremental PCA is useful for large datasets that donot fit in memeory. But it is slower than Regular PCA, So if the dataset fits in memory you should prefer regular PCA.Incremental PCA is also used for online tasks When you need to apply PCA on the fly every time new insatnce arrives.\n",
    "Randomizes PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory, In this case it is must faster than regular PCA.\n",
    "Finally kernel PCA is useful for nonlinear dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "97df5fde",
   "metadata": {},
   "source": [
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9c5d960",
   "metadata": {},
   "source": [
    "Intutively , a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from dataset without losing too much info.\n",
    "One way to measure this is to apply the reverse transformation and measured reconstruction error.\n",
    "however not all dimensionality reduction algorithms provide reverse transformation.\n",
    "Alternatively if you are using dimensionality reduction as preprocessing step before another machine learning algorithm.e.g RFC , Then you can simply measure the performance of that second algorithm , If dimensionality reduction did not lose too much then the algorithm should perform just as well as when using the original dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0b15ed2",
   "metadata": {},
   "source": [
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56e1ab4e",
   "metadata": {},
   "source": [
    "It can absolutely make sense chain two different dimensionality reduction algorithms.\n",
    "A common example of using PCA to quickly get rid of a large number useless dimensions , Then applying another much slower dimensionality reduction algorithm such as LLF.This two step approach will likely yield the same prformance as using LLE only, but in fraction of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df618be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
